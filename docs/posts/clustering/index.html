<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jonathan West">
<meta name="dcterms.date" content="2023-11-22">
<meta name="description" content="This blog post demonstrates the concept of clustering in machine learning.">

<title>ML Blog - Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">ML Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/jrgranadoswest/ml_blog" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Clustering</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          This blog post demonstrates the concept of clustering in machine learning.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Clustering</div>
                <div class="quarto-category">code</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Jonathan West </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 22, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Clustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. The process is essential for finding patterns in data and is used in a variety of fields. It is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself.</p>
<p>Today we will be demonstrating use of a couple different clustering algorithms.</p>
</section>
<section id="k-means" class="level2">
<h2 class="anchored" data-anchor-id="k-means">K-Means</h2>
<p>K-Means clustering is a clustering algorithm which aims to partition “n” observations into “k” different groups. As the name suggests, the grouping of each observation is determined by mean of the cluster. Essentially, the algorithm finds the mean of each cluster and assigns each observation to the cluster with the nearest mean, repeatedly recomputing the means until the clusters no longer change.</p>
<p>Lets exemplify this process with a fun example. We are going to use the K-Means algorithm to group the colors of an image into clusters. First, lets load an image.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the image, convert to RGB</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(<span class="st">'landscape.jpg'</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshaping the image into a 2D array of pixels and 3 float values (RGB)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> image.reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> np.float32(pixels)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f0fc556bf90&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-2.png" width="575" height="297"></p>
</div>
</div>
<p>This image is a matrix of pixels, where the pixels are represented by values that give their color based on the values of Red, Green, and Blue. Therefore each pixel can be considered as a point in a 3D color space, and we can use the K-Means algorithm to group the pixels in this space. To be specific, each pixel will be grouped with the cluster whose mean is closest to the pixel’s color, and these means will be updated until the clusters no longer change.</p>
<p>Then, we can display the image with each pixel represented by the mean color of its cluster.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run k-means with random initial centers, k=3 clusters, stopping at 100 iterations or an epsilon value of 90%</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>criteria <span class="op">=</span> (cv2.TERM_CRITERIA_EPS <span class="op">+</span> cv2.TERM_CRITERIA_MAX_ITER, <span class="dv">100</span>, <span class="fl">0.9</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>retval, labels, centers <span class="op">=</span> cv2.kmeans(pixels, k, <span class="va">None</span>, criteria, <span class="dv">10</span>, cv2.KMEANS_RANDOM_CENTERS)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># convert data into 8-bit values, and reshape to original image dimensions</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> np.uint8(centers)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>segmented_data <span class="op">=</span> centers[labels.flatten()]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>segmented_image <span class="op">=</span> segmented_data.reshape((image.shape))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display modified image </span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(segmented_image)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f0fc4026650&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-2.png" width="575" height="297"></p>
</div>
</div>
<p>Here we can see the image after having run the K-Means algorithm with K=6 clusters. You can probably think of some use cases for this, such as image compression, a stylistic effect, or simplifying an image for computer vision work or other analysis.</p>
<p>To further illustrate how k-means works, lets manually implement the algorithm on this data. Essentially we will be doing the same thing that the above code does: we initialize “k” different cluster centroids, assign each point to the cluster with the closest centroid, and then update the centroids to be the mean of the points in each cluster. We will keep repeating this process until the centroids no longer change between iterations. We start by initializing the centroids to random points in the color space. In practice we may want to ensure that the centroids are initialized to be far apart from each other or randomly sample existing colors in the image, but for simplicity we will just initialize them randomly. We also create a labels array to keep track of which cluster each pixel belongs to.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Random number between 0 and 255:</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> np.array([[random.randint(<span class="dv">0</span>, <span class="dv">255</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(k)])</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(means)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.zeros(pixels.shape[<span class="dv">0</span>], dtype<span class="op">=</span>np.uint8)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pixels.shape)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(labels.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[127  16  49]
 [ 86  68 218]
 [209  43 100]
 [203 171 224]]
(110880, 3)
(110880,)</code></pre>
</div>
</div>
<p>Next, let’s make a quick helper method that will reobtain the image from the pixels array and current cluster means.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_image(pixels, means):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    new_pixels <span class="op">=</span> np.zeros(pixels.shape, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, px <span class="kw">in</span> <span class="bu">enumerate</span>(pixels):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        new_pixels[i] <span class="op">=</span> means[labels[i]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    new_image <span class="op">=</span> new_pixels.reshape((image.shape))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_image</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>init_image <span class="op">=</span> get_image(pixels, means)  <span class="co"># image with initial random means</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we will implement the main program loop. For the sake of simplicity we will simply run 8 iterations, but in practice we would want to run until the centroids no longer change, or a certain epsilon value is reached.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over all pixels and assign them to the closest cluster</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> idx <span class="op">&lt;</span> <span class="dv">8</span>:</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over all pixels and update their cluster assignments</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, px <span class="kw">in</span> <span class="bu">enumerate</span>(pixels):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute euclidian distance (the l2 norm) to each cluster mean</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> [np.linalg.norm(px <span class="op">-</span> mean) <span class="cf">for</span> mean <span class="kw">in</span> means]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># New cluster is the one with the smallest distance</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        cluster <span class="op">=</span> np.argmin(distances)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        labels[i] <span class="op">=</span> cluster</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        init_image <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> idx <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        image_1_iter <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        image_3_iter <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update cluster means</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find average of all pixels from "pixels" where the label corresponds to the current cluster mean</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    old_means <span class="op">=</span> np.copy(means)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>        means[i] <span class="op">=</span> np.mean(pixels[labels <span class="op">==</span> i], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute difference between current and previous cluster means</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(means)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[ 59  62  45]
 [ 81 111 114]
 [130 107  63]
 [195 171 150]]</code></pre>
</div>
</div>
<p>Finally, let’s display the image results. We can display the image with each pixel represented by the mean color of its cluster, and show how the means changed over 0, 10, 50 and 100 iterations.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>final_image <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">1</span>, title<span class="op">=</span><span class="st">"Initial Clusters"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>plt.imshow(init_image)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">2</span>, title<span class="op">=</span><span class="st">"After 1 Iteration"</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>plt.imshow(image_1_iter)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">3</span>, title<span class="op">=</span><span class="st">"After 3 Iterations"</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(image_3_iter)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">4</span>, title<span class="op">=</span><span class="st">"Final Clusters"</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>plt.imshow(final_image)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="649" height="514"></p>
</div>
</div>
<p>Now let’s consider a more practical dataset for clustering. We are going to look at the scikit learn wine dataset, and work to cluster the wine samples into groups based on their features.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Wine dataset</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>wine <span class="op">=</span> load_wine()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> wine.data</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> wine.target  <span class="co"># True class labels</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Wine Dataset:"</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(wine.target_names)<span class="sc">}</span><span class="ss"> Target names: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(wine.target_names)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(wine.feature_names)<span class="sc">}</span><span class="ss"> Feature names: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(wine.feature_names)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data shape: </span><span class="sc">{</span>wine<span class="sc">.</span>data<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wine.data[:<span class="dv">5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Wine Dataset:
3 Target names: class_0, class_1, class_2
13 Feature names: alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline
Data shape: (178, 13)
[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00
  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]
 [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00
  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]
 [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00
  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]
 [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00
  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]
 [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00
  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]</code></pre>
</div>
</div>
<p>The dataset contains 13 features, and the target class includes 3 different types of wine. Note that because this data is not 2-dimensional, we will need to use a dimensionality reduction technique to visualize the clusters in a 2D plot. Specifically, we will use PCA, or principle component analysis, to reduce the dimensionality of the data to 2D.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA for dimensionality reduction, so we can visualize data with 2D graph</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform k-means clustering</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">3</span>  <span class="co"># You can change the number of clusters as needed</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>kmeans.fit(X)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Get cluster labels and cluster centers</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> kmeans.labels_</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> kmeans.cluster_centers_</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points colored by the cluster labels</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>labels, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'K-Means Clustering Results'</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/jrgw/CS5805_ML/blogposts/blogwork/jrgwblog/venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-2.png" width="686" height="523"></p>
</div>
</div>
<p>As we did before, we ran k-means clustering on the data, this time with k=3 clusters. To plot the results we used principle component analysis (PCA) to reduce the dimensionality of the data to 2D, so we could visualize the clusters in a scatter plot.</p>
<p>Next, lets plot the data with the three true classification labels they were provided with in the dataset, to compare to the unsupervised learning groups.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points colored by the true class labels</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>y_true, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'True Class Labels'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="686" height="523"></p>
</div>
</div>
<p>As we can see from comparing the two graphs, the clusters found by the k-means algorithm are comparable to the true class labels, but not quite the same. Next, lets consider how the choice of K affects our results. In this particular case, we know that there are three true wine classes, so we would expect that the best choice of K would be 3. However in practice the number of classes based on what we are trying to accomplish may not be known, so we may need to experiment with different values of K to find the best one.</p>
<p>With the k-means algorithm, “inertia” is a measure that quantifies the compactness or tightness of the clusters. It calculates the sum of the squared distances between each data point and its corresponding centroid in its cluster. Inertia allows us to assess how well the data points are grouped into clusters, because lower inertia values indicate less distance from points to their cluster centroids, and therefore the clusters are more compact.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>kmeans_per_k <span class="op">=</span> [KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit(wine.data) <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">10</span>)]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>inertias <span class="op">=</span> [model.inertia_ <span class="cf">for</span> model <span class="kw">in</span> kmeans_per_k]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.5</span>))</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">10</span>), inertias, <span class="st">"bo-"</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Elbow Curve"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$k$"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Inertia"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-11-output-1.png" width="672" height="339"></p>
</div>
</div>
<p>We call this graph an “Elbow Curve”, because the optimal value of K is often the point where the inertia curve has an “elbow” or sharp turn. As you can see, as our choice of k increases, the inertia decreases because more data points allow us to have more compact clusters. This will be true all the way until we reach k=n observations, where each observation is its own cluster and the inertia is 0. However, as you may have realized, this is not a good choice of k because it does not allow us to group similar observations together. The optimal value of k is the point where the inertia curve has an “elbow” or sharp turn, because this is the point where the inertia decreases more slowly as k increases, and therefore the point where the clusters are most compact, while still meaningfully grouping the data.</p>
<p>In this case, it appears that the optimal value of k is 2 rather than the 3 true classes, because the inertia curve has a sharp turn at k=2, and then decreases more slowly as k increases. This may be due to the particular criteria of wine classes not perfectly matching the wine features that we are grouping based on, or it may be due to the fact that the wine classes are not perfectly distinct from each other. In practice, if we did not have the true class labels, then we would likely choose k=2 in this case.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this blog post, we have jumped into the concept of clustering in machine learning, which involves using unsupervised learning to find patterns and groups in unlabeled data. We discussed k-means and how to implement it, and why it is used. By understanding the concept of inertia and its pivotal role in k-means optimization, we gained insights into how to best make use of it. Clustering is an essential technique with diverse applications, allowing us to extract meaningful patterns and structures from complex datasets, making it an essential tool in a machine learning practitioner’s toolbox. Hopefully you can use this blog as an inspiration to explore clustering further, for example by trying other clustering algorithms such as DBSCAN or Gaussian Mixture Models, or by applying clustering to your own datasets.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li>https://thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python</li>
<li>https://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/</li>
<li>https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html</li>
<li>https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html</li>
<li>https://github.com/ageron/handson-ml3</li>
</ol>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb17" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Clustering"</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Jonathan West"</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-22"</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Clustering, code]</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "This blog post demonstrates the concept of clustering in machine learning."</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "clustering.png"</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co">    html:</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co">        code-tools: true</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>Clustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. </span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>The process is essential for finding patterns in data and is used in a variety of fields. </span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>It is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself. </span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>Today we will be demonstrating use of a couple different clustering algorithms.</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a><span class="fu">## K-Means</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>K-Means clustering is a clustering algorithm which aims to partition "n" observations into "k" different groups. </span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>As the name suggests, the grouping of each observation is determined by mean of the cluster. </span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>Essentially, the algorithm finds the mean of each cluster and assigns each observation to the cluster with the nearest mean, repeatedly recomputing the means until the clusters no longer change.</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>Lets exemplify this process with a fun example. We are going to use the K-Means algorithm to group the colors of an image into clusters. First, lets load an image.</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cv2</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Read in the image, convert to RGB</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.imread(<span class="st">'landscape.jpg'</span>)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>image <span class="op">=</span> cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshaping the image into a 2D array of pixels and 3 float values (RGB)</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> image.reshape((<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>))</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>pixels <span class="op">=</span> np.float32(pixels)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Display image</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>plt.imshow(image)</span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>This image is a matrix of pixels, where the pixels are represented by values that give their color based on the values of Red, Green, and Blue. </span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>Therefore each pixel can be considered as a point in a 3D color space, and we can use the K-Means algorithm to group the pixels in this space.</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>To be specific, each pixel will be grouped with the cluster whose mean is closest to the pixel's color, and these means will be updated until the clusters no longer change.</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a>Then, we can display the image with each pixel represented by the mean color of its cluster.</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a><span class="co"># run k-means with random initial centers, k=3 clusters, stopping at 100 iterations or an epsilon value of 90%</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>criteria <span class="op">=</span> (cv2.TERM_CRITERIA_EPS <span class="op">+</span> cv2.TERM_CRITERIA_MAX_ITER, <span class="dv">100</span>, <span class="fl">0.9</span>)</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>retval, labels, centers <span class="op">=</span> cv2.kmeans(pixels, k, <span class="va">None</span>, criteria, <span class="dv">10</span>, cv2.KMEANS_RANDOM_CENTERS)</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a><span class="co"># convert data into 8-bit values, and reshape to original image dimensions</span></span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> np.uint8(centers)</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>segmented_data <span class="op">=</span> centers[labels.flatten()]</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>segmented_image <span class="op">=</span> segmented_data.reshape((image.shape))</span>
<span id="cb17-64"><a href="#cb17-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-65"><a href="#cb17-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Display modified image </span></span>
<span id="cb17-66"><a href="#cb17-66" aria-hidden="true" tabindex="-1"></a>plt.imshow(segmented_image)</span>
<span id="cb17-67"><a href="#cb17-67" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-68"><a href="#cb17-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-69"><a href="#cb17-69" aria-hidden="true" tabindex="-1"></a>Here we can see the image after having run the K-Means algorithm with K=6 clusters. </span>
<span id="cb17-70"><a href="#cb17-70" aria-hidden="true" tabindex="-1"></a>You can probably think of some use cases for this, such as image compression, a stylistic effect, or simplifying an image for computer vision work or other analysis.</span>
<span id="cb17-71"><a href="#cb17-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-72"><a href="#cb17-72" aria-hidden="true" tabindex="-1"></a>To further illustrate how k-means works, lets manually implement the algorithm on this data. Essentially we will be doing the same thing that the above code does: we initialize "k" different </span>
<span id="cb17-73"><a href="#cb17-73" aria-hidden="true" tabindex="-1"></a>cluster centroids, assign each point to the cluster with the closest centroid, and then update the centroids to be the mean of the points in each cluster. </span>
<span id="cb17-74"><a href="#cb17-74" aria-hidden="true" tabindex="-1"></a>We will keep repeating this process until the centroids no longer change between iterations. </span>
<span id="cb17-75"><a href="#cb17-75" aria-hidden="true" tabindex="-1"></a>We start by initializing the centroids to random points in the color space. In practice we may want to ensure that the centroids are initialized to be far apart from each other or randomly sample existing colors in the image, but for simplicity we will just initialize them randomly. We also create a labels array to keep track of which cluster each pixel belongs to.</span>
<span id="cb17-76"><a href="#cb17-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-79"><a href="#cb17-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-80"><a href="#cb17-80" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb17-81"><a href="#cb17-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-82"><a href="#cb17-82" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb17-83"><a href="#cb17-83" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">42</span>)</span>
<span id="cb17-84"><a href="#cb17-84" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb17-85"><a href="#cb17-85" aria-hidden="true" tabindex="-1"></a><span class="co"># Random number between 0 and 255:</span></span>
<span id="cb17-86"><a href="#cb17-86" aria-hidden="true" tabindex="-1"></a>num <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="cb17-87"><a href="#cb17-87" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> np.array([[random.randint(<span class="dv">0</span>, <span class="dv">255</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>)] <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(k)])</span>
<span id="cb17-88"><a href="#cb17-88" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(means)</span>
<span id="cb17-89"><a href="#cb17-89" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.zeros(pixels.shape[<span class="dv">0</span>], dtype<span class="op">=</span>np.uint8)</span>
<span id="cb17-90"><a href="#cb17-90" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pixels.shape)</span>
<span id="cb17-91"><a href="#cb17-91" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(labels.shape)</span>
<span id="cb17-92"><a href="#cb17-92" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-93"><a href="#cb17-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-94"><a href="#cb17-94" aria-hidden="true" tabindex="-1"></a>Next, let's make a quick helper method that will reobtain the image from the pixels array and current cluster means.</span>
<span id="cb17-95"><a href="#cb17-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-98"><a href="#cb17-98" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-99"><a href="#cb17-99" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_image(pixels, means):</span>
<span id="cb17-100"><a href="#cb17-100" aria-hidden="true" tabindex="-1"></a>    new_pixels <span class="op">=</span> np.zeros(pixels.shape, dtype<span class="op">=</span>np.uint8)</span>
<span id="cb17-101"><a href="#cb17-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, px <span class="kw">in</span> <span class="bu">enumerate</span>(pixels):</span>
<span id="cb17-102"><a href="#cb17-102" aria-hidden="true" tabindex="-1"></a>        new_pixels[i] <span class="op">=</span> means[labels[i]]</span>
<span id="cb17-103"><a href="#cb17-103" aria-hidden="true" tabindex="-1"></a>    new_image <span class="op">=</span> new_pixels.reshape((image.shape))</span>
<span id="cb17-104"><a href="#cb17-104" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_image</span>
<span id="cb17-105"><a href="#cb17-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-106"><a href="#cb17-106" aria-hidden="true" tabindex="-1"></a>init_image <span class="op">=</span> get_image(pixels, means)  <span class="co"># image with initial random means</span></span>
<span id="cb17-107"><a href="#cb17-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-108"><a href="#cb17-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-109"><a href="#cb17-109" aria-hidden="true" tabindex="-1"></a>Now, we will implement the main program loop. For the sake of simplicity we will simply run 8 iterations, but in practice we would want to run until the centroids no longer change, or a certain epsilon value is reached.</span>
<span id="cb17-110"><a href="#cb17-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-113"><a href="#cb17-113" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-114"><a href="#cb17-114" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate over all pixels and assign them to the closest cluster</span></span>
<span id="cb17-115"><a href="#cb17-115" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb17-116"><a href="#cb17-116" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> idx <span class="op">&lt;</span> <span class="dv">8</span>:</span>
<span id="cb17-117"><a href="#cb17-117" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate over all pixels and update their cluster assignments</span></span>
<span id="cb17-118"><a href="#cb17-118" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, px <span class="kw">in</span> <span class="bu">enumerate</span>(pixels):</span>
<span id="cb17-119"><a href="#cb17-119" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute euclidian distance (the l2 norm) to each cluster mean</span></span>
<span id="cb17-120"><a href="#cb17-120" aria-hidden="true" tabindex="-1"></a>        distances <span class="op">=</span> [np.linalg.norm(px <span class="op">-</span> mean) <span class="cf">for</span> mean <span class="kw">in</span> means]</span>
<span id="cb17-121"><a href="#cb17-121" aria-hidden="true" tabindex="-1"></a>        <span class="co"># New cluster is the one with the smallest distance</span></span>
<span id="cb17-122"><a href="#cb17-122" aria-hidden="true" tabindex="-1"></a>        cluster <span class="op">=</span> np.argmin(distances)</span>
<span id="cb17-123"><a href="#cb17-123" aria-hidden="true" tabindex="-1"></a>        labels[i] <span class="op">=</span> cluster</span>
<span id="cb17-124"><a href="#cb17-124" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb17-125"><a href="#cb17-125" aria-hidden="true" tabindex="-1"></a>        init_image <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb17-126"><a href="#cb17-126" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> idx <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb17-127"><a href="#cb17-127" aria-hidden="true" tabindex="-1"></a>        image_1_iter <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb17-128"><a href="#cb17-128" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> idx <span class="op">==</span> <span class="dv">3</span>:</span>
<span id="cb17-129"><a href="#cb17-129" aria-hidden="true" tabindex="-1"></a>        image_3_iter <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb17-130"><a href="#cb17-130" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update cluster means</span></span>
<span id="cb17-131"><a href="#cb17-131" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find average of all pixels from "pixels" where the label corresponds to the current cluster mean</span></span>
<span id="cb17-132"><a href="#cb17-132" aria-hidden="true" tabindex="-1"></a>    old_means <span class="op">=</span> np.copy(means)</span>
<span id="cb17-133"><a href="#cb17-133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb17-134"><a href="#cb17-134" aria-hidden="true" tabindex="-1"></a>        means[i] <span class="op">=</span> np.mean(pixels[labels <span class="op">==</span> i], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-135"><a href="#cb17-135" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute difference between current and previous cluster means</span></span>
<span id="cb17-136"><a href="#cb17-136" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb17-137"><a href="#cb17-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-138"><a href="#cb17-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(means)</span>
<span id="cb17-139"><a href="#cb17-139" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-140"><a href="#cb17-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-141"><a href="#cb17-141" aria-hidden="true" tabindex="-1"></a>Finally, let's display the image results. We can display the image with each pixel represented by the mean color of its cluster, and show how the means changed over 0, 10, 50 and 100 iterations.</span>
<span id="cb17-142"><a href="#cb17-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-145"><a href="#cb17-145" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-146"><a href="#cb17-146" aria-hidden="true" tabindex="-1"></a>final_image <span class="op">=</span> get_image(pixels, means)</span>
<span id="cb17-147"><a href="#cb17-147" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb17-148"><a href="#cb17-148" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-149"><a href="#cb17-149" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb17-150"><a href="#cb17-150" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">1</span>, title<span class="op">=</span><span class="st">"Initial Clusters"</span>)</span>
<span id="cb17-151"><a href="#cb17-151" aria-hidden="true" tabindex="-1"></a>plt.imshow(init_image)</span>
<span id="cb17-152"><a href="#cb17-152" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">2</span>, title<span class="op">=</span><span class="st">"After 1 Iteration"</span>)</span>
<span id="cb17-153"><a href="#cb17-153" aria-hidden="true" tabindex="-1"></a>plt.imshow(image_1_iter)</span>
<span id="cb17-154"><a href="#cb17-154" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">3</span>, title<span class="op">=</span><span class="st">"After 3 Iterations"</span>)</span>
<span id="cb17-155"><a href="#cb17-155" aria-hidden="true" tabindex="-1"></a>plt.imshow(image_3_iter)</span>
<span id="cb17-156"><a href="#cb17-156" aria-hidden="true" tabindex="-1"></a>fig.add_subplot(rows, columns, <span class="dv">4</span>, title<span class="op">=</span><span class="st">"Final Clusters"</span>)</span>
<span id="cb17-157"><a href="#cb17-157" aria-hidden="true" tabindex="-1"></a>plt.imshow(final_image)</span>
<span id="cb17-158"><a href="#cb17-158" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-159"><a href="#cb17-159" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-160"><a href="#cb17-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-161"><a href="#cb17-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-162"><a href="#cb17-162" aria-hidden="true" tabindex="-1"></a>Now let's consider a more practical dataset for clustering. We are going to look at the scikit learn wine dataset, and work to cluster the wine samples into groups based on their features.</span>
<span id="cb17-163"><a href="#cb17-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-166"><a href="#cb17-166" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-167"><a href="#cb17-167" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_wine</span>
<span id="cb17-168"><a href="#cb17-168" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb17-169"><a href="#cb17-169" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb17-170"><a href="#cb17-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-171"><a href="#cb17-171" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the Wine dataset</span></span>
<span id="cb17-172"><a href="#cb17-172" aria-hidden="true" tabindex="-1"></a>wine <span class="op">=</span> load_wine()</span>
<span id="cb17-173"><a href="#cb17-173" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> wine.data</span>
<span id="cb17-174"><a href="#cb17-174" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> wine.target  <span class="co"># True class labels</span></span>
<span id="cb17-175"><a href="#cb17-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-176"><a href="#cb17-176" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Wine Dataset:"</span>)</span>
<span id="cb17-177"><a href="#cb17-177" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(wine.target_names)<span class="sc">}</span><span class="ss"> Target names: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(wine.target_names)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-178"><a href="#cb17-178" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(wine.feature_names)<span class="sc">}</span><span class="ss"> Feature names: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(wine.feature_names)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-179"><a href="#cb17-179" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Data shape: </span><span class="sc">{</span>wine<span class="sc">.</span>data<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-180"><a href="#cb17-180" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(wine.data[:<span class="dv">5</span>])</span>
<span id="cb17-181"><a href="#cb17-181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-182"><a href="#cb17-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-183"><a href="#cb17-183" aria-hidden="true" tabindex="-1"></a>The dataset contains 13 features, and the target class includes 3 different types of wine.</span>
<span id="cb17-184"><a href="#cb17-184" aria-hidden="true" tabindex="-1"></a>Note that because this data is not 2-dimensional, we will need to use a dimensionality reduction technique to visualize the clusters in a 2D plot. </span>
<span id="cb17-185"><a href="#cb17-185" aria-hidden="true" tabindex="-1"></a>Specifically, we will use PCA, or principle component analysis, to reduce the dimensionality of the data to 2D.</span>
<span id="cb17-186"><a href="#cb17-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-187"><a href="#cb17-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-190"><a href="#cb17-190" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-191"><a href="#cb17-191" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply PCA for dimensionality reduction, so we can visualize data with 2D graph</span></span>
<span id="cb17-192"><a href="#cb17-192" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-193"><a href="#cb17-193" aria-hidden="true" tabindex="-1"></a>X_pca <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb17-194"><a href="#cb17-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-195"><a href="#cb17-195" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform k-means clustering</span></span>
<span id="cb17-196"><a href="#cb17-196" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">3</span>  <span class="co"># You can change the number of clusters as needed</span></span>
<span id="cb17-197"><a href="#cb17-197" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-198"><a href="#cb17-198" aria-hidden="true" tabindex="-1"></a>kmeans.fit(X)</span>
<span id="cb17-199"><a href="#cb17-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-200"><a href="#cb17-200" aria-hidden="true" tabindex="-1"></a><span class="co"># Get cluster labels and cluster centers</span></span>
<span id="cb17-201"><a href="#cb17-201" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> kmeans.labels_</span>
<span id="cb17-202"><a href="#cb17-202" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> kmeans.cluster_centers_</span>
<span id="cb17-203"><a href="#cb17-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-204"><a href="#cb17-204" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb17-205"><a href="#cb17-205" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points colored by the cluster labels</span></span>
<span id="cb17-206"><a href="#cb17-206" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>labels, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb17-207"><a href="#cb17-207" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'K-Means Clustering Results'</span>)</span>
<span id="cb17-208"><a href="#cb17-208" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb17-209"><a href="#cb17-209" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb17-210"><a href="#cb17-210" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-211"><a href="#cb17-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-212"><a href="#cb17-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-213"><a href="#cb17-213" aria-hidden="true" tabindex="-1"></a>As we did before, we ran k-means clustering on the data, this time with k=3 clusters. </span>
<span id="cb17-214"><a href="#cb17-214" aria-hidden="true" tabindex="-1"></a>To plot the results we used principle component analysis (PCA) to reduce the dimensionality of the data to 2D, so we could visualize the clusters in a scatter plot.</span>
<span id="cb17-215"><a href="#cb17-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-216"><a href="#cb17-216" aria-hidden="true" tabindex="-1"></a>Next, lets plot the data with the three true classification labels they were provided with in the dataset, to compare to the unsupervised learning groups.</span>
<span id="cb17-217"><a href="#cb17-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-220"><a href="#cb17-220" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-221"><a href="#cb17-221" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb17-222"><a href="#cb17-222" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data points colored by the true class labels</span></span>
<span id="cb17-223"><a href="#cb17-223" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_pca[:, <span class="dv">0</span>], X_pca[:, <span class="dv">1</span>], c<span class="op">=</span>y_true, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb17-224"><a href="#cb17-224" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'True Class Labels'</span>)</span>
<span id="cb17-225"><a href="#cb17-225" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Principal Component 1'</span>)</span>
<span id="cb17-226"><a href="#cb17-226" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Principal Component 2'</span>)</span>
<span id="cb17-227"><a href="#cb17-227" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-228"><a href="#cb17-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-229"><a href="#cb17-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-230"><a href="#cb17-230" aria-hidden="true" tabindex="-1"></a>As we can see from comparing the two graphs, the clusters found by the k-means algorithm are comparable to the true class labels, but not quite the same.</span>
<span id="cb17-231"><a href="#cb17-231" aria-hidden="true" tabindex="-1"></a>Next, lets consider how the choice of K affects our results. In this particular case, we know that there are three true wine classes, so we would expect that the best choice of K would be 3.</span>
<span id="cb17-232"><a href="#cb17-232" aria-hidden="true" tabindex="-1"></a>However in practice the number of classes based on what we are trying to accomplish may not be known, so we may need to experiment with different values of K to find the best one.</span>
<span id="cb17-233"><a href="#cb17-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-234"><a href="#cb17-234" aria-hidden="true" tabindex="-1"></a>With the k-means algorithm, "inertia" is a measure that quantifies the compactness or tightness of the clusters. It calculates the sum of the squared distances between each data point and its corresponding centroid in its cluster. Inertia allows us to assess how well the data points are grouped into clusters, because lower inertia values indicate less distance from points to their cluster centroids, and therefore the clusters are more compact. </span>
<span id="cb17-235"><a href="#cb17-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-238"><a href="#cb17-238" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb17-239"><a href="#cb17-239" aria-hidden="true" tabindex="-1"></a>kmeans_per_k <span class="op">=</span> [KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>).fit(wine.data) <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">10</span>)]</span>
<span id="cb17-240"><a href="#cb17-240" aria-hidden="true" tabindex="-1"></a>inertias <span class="op">=</span> [model.inertia_ <span class="cf">for</span> model <span class="kw">in</span> kmeans_per_k]</span>
<span id="cb17-241"><a href="#cb17-241" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">3.5</span>))</span>
<span id="cb17-242"><a href="#cb17-242" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, <span class="dv">10</span>), inertias, <span class="st">"bo-"</span>)</span>
<span id="cb17-243"><a href="#cb17-243" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Elbow Curve"</span>)</span>
<span id="cb17-244"><a href="#cb17-244" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$k$"</span>)</span>
<span id="cb17-245"><a href="#cb17-245" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Inertia"</span>)</span>
<span id="cb17-246"><a href="#cb17-246" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb17-247"><a href="#cb17-247" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb17-248"><a href="#cb17-248" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb17-249"><a href="#cb17-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-250"><a href="#cb17-250" aria-hidden="true" tabindex="-1"></a>We call this graph an "Elbow Curve", because the optimal value of K is often the point where the inertia curve has an "elbow" or sharp turn. </span>
<span id="cb17-251"><a href="#cb17-251" aria-hidden="true" tabindex="-1"></a>As you can see, as our choice of k increases, the inertia decreases because more data points allow us to have more compact clusters.</span>
<span id="cb17-252"><a href="#cb17-252" aria-hidden="true" tabindex="-1"></a>This will be true all the way until we reach k=n observations, where each observation is its own cluster and the inertia is 0.</span>
<span id="cb17-253"><a href="#cb17-253" aria-hidden="true" tabindex="-1"></a>However, as you may have realized, this is not a good choice of k because it does not allow us to group similar observations together.</span>
<span id="cb17-254"><a href="#cb17-254" aria-hidden="true" tabindex="-1"></a>The optimal value of k is the point where the inertia curve has an "elbow" or sharp turn, because this is the point where the inertia decreases more slowly as k increases, and therefore the point where the clusters are most compact, while still meaningfully grouping the data.</span>
<span id="cb17-255"><a href="#cb17-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-256"><a href="#cb17-256" aria-hidden="true" tabindex="-1"></a>In this case, it appears that the optimal value of k is 2 rather than the 3 true classes, because the inertia curve has a sharp turn at k=2, and then decreases more slowly as k increases. This may be due to the particular criteria of wine classes not perfectly matching the wine features that we are grouping based on, or it may be due to the fact that the wine classes are not perfectly distinct from each other. In practice, if we did not have the true class labels, then we would likely choose k=2 in this case.</span>
<span id="cb17-257"><a href="#cb17-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-258"><a href="#cb17-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-259"><a href="#cb17-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb17-260"><a href="#cb17-260" aria-hidden="true" tabindex="-1"></a>In this blog post, we have jumped into the concept of clustering in machine learning, which involves using unsupervised learning to find patterns and groups in unlabeled data. We discussed k-means and how to implement it, and why it is used. By understanding the concept of inertia and its pivotal role in k-means optimization, we gained insights into how to best make use of it. Clustering is an essential technique with diverse applications, allowing us to extract meaningful patterns and structures from complex datasets, making it an essential tool in a machine learning practitioner's toolbox. Hopefully you can use this blog as an inspiration to explore clustering further, for example by trying other clustering algorithms such as DBSCAN or Gaussian Mixture Models, or by applying clustering to your own datasets.</span>
<span id="cb17-261"><a href="#cb17-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-262"><a href="#cb17-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-263"><a href="#cb17-263" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb17-264"><a href="#cb17-264" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>https://thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python</span>
<span id="cb17-265"><a href="#cb17-265" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>https://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/</span>
<span id="cb17-266"><a href="#cb17-266" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html</span>
<span id="cb17-267"><a href="#cb17-267" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html</span>
<span id="cb17-268"><a href="#cb17-268" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>https://github.com/ageron/handson-ml3</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>