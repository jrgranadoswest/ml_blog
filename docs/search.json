[
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. The process is essential for finding patterns in data and is used in a variety of fields. It is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself.\nToday we will be demonstrating use of a couple different clustering algorithms."
  },
  {
    "objectID": "posts/clustering/index.html#introduction",
    "href": "posts/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. The process is essential for finding patterns in data and is used in a variety of fields. It is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself.\nToday we will be demonstrating use of a couple different clustering algorithms."
  },
  {
    "objectID": "posts/clustering/index.html#conclusion",
    "href": "posts/clustering/index.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have jumped into the concept of clustering in machine learning, which involves using unsupervised learning to find patterns and groups in unlabeled data. We discussed k-means and how to implement it, and why it is used. By understanding the concept of inertia and its pivotal role in k-means optimization, we gained insights into how to best make use of it. Clustering is an essential technique with diverse applications, allowing us to extract meaningful patterns and structures from complex datasets, making it an essential tool in a machine learning practitioner’s toolbox. Hopefully you can use this blog as an inspiration to explore clustering further, for example by trying other clustering algorithms such as DBSCAN or Gaussian Mixture Models, or by applying clustering to your own datasets."
  },
  {
    "objectID": "posts/prob_theory_rv/index.html",
    "href": "posts/prob_theory_rv/index.html",
    "title": "Probability Theory & Random Variables",
    "section": "",
    "text": "Probability theory is a fundamental component of machine learning, as it covers the study of uncertainty and randomness. It entails the work of making predictions in the absence of complete information. In this blog post we will walk through some concepts of probability theory, show how probability theory and random variables will be important moving forward in our study of machine learning. In particular, we will discuss logistic regression and naive bayes classifiers.\n\n\nOne key aspect of probability theory is the probability distribution, which is essentially a mathematical function that provides the probabilities of occurrence of different possible outcomes in a given scenario. For instance, if we were to flip a coin, the probability distribution would be 50% heads and 50% tails. To get started, we will generate two sets of random data, to demonstrate the uniform and normal probability distributions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data from two distributions\ndata_unif = np.random.uniform(0, 1, 800)  # Uniform\ndata_norm = np.random.normal(0, 1, 800)  # Normal\n\nplt.figure(figsize=(6, 8))  # Create figure with two subplots\n\n# Uniform Distribution\nplt.subplot(2, 1, 1)\nplt.hist(data_unif, bins=20, density=True, alpha=0.6, color='green', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Uniform Probability Distribution')\n\n# Normal Distribution\nplt.subplot(2, 1, 2)\nplt.hist(data_norm, bins=20, density=True, alpha=0.6, color='purple', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Normal Probability Distribution')\n\nplt.tight_layout()  # Avoid overlapping graphs\nplt.show()  # Display\n\n\n\n\nAs you can see above, the uniform distribution appears as a rough rectangle, while the normal distribution appears as a bell curve. These are two common probability distributions, where outcomes are all equally likely, and where outcomes are more likely to be near the mean, respectively. Next lets apply some probability theory to a more practical set of data.\n\n\n\nFirst we will load the iris dataset from scikit-learn, and examine its features.\n\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris_data = load_iris()\n# Display some basic information about the dataset\nprint(\"Iris Dataset:\")\nprint(f\"{len(iris_data.target_names)} Target names: {', '.join(iris_data.target_names)}\")\nprint(f\"{len(iris_data.feature_names)} Feature names: {', '.join(iris_data.feature_names)}\")\nprint(f\"Data shape: {iris_data.data.shape}\")\n\nIris Dataset:\n3 Target names: setosa, versicolor, virginica\n4 Feature names: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)\nData shape: (150, 4)\n\n\n\nimport pandas as pd\n# Display first 5 rows of data\ndf = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\ndf[\"target\"] = iris_data.target\n# Iterate over data frame and replace target values with target names\nfor target, target_name in enumerate(iris_data.target_names):\n    df[\"target\"] = df[\"target\"].replace(target, target_name)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAs shown above, this dataset contains 150 observations, with 4 features, and 3 target classes. By loading the dataset into a pandas dataframe, we can easily visualize the data and its features.\nFirst, lets visualize the data with a kernel density estimate (KDE) plot, which allows us to visualize the distribution of observations in a dataset. The KDE plot is a smoothed version of the histogram, which shows the probability density function (PDF) of the data.\n\nimport seaborn as sns\n# KDE plot of data \nsns.displot(df, x=\"sepal length (cm)\", kde=True, hue=\"target\", stat=\"density\")\n\n\n\n\nAbove we can see the KDE plot of the sepal length feature, with the target classes colored. The “stat” argument, in this case set to “density”, means that the KDE plot will be normalized such that the area under the curve is equal to 1. This graph allows us to understand the probability distribution of our data in terms of the target classes. We can see that the setosa class has a much smaller sepal length than the other two classes.\n\nsns.displot(df, x=\"sepal width (cm)\", kde=True, hue=\"target\", stat=\"count\")\n\n\n\n\nWith this second graph, we can see that the sepal width feature is not as useful for distinguishing between the target classes, as the distributions are very similar. However, the setosa class does appear to have a slightly larger sepal width than the other two, and the versicolor class appears to have a slightly smaller sepal width than the other two. By accounting for all of the features, we can create a more accurate model for predicting the target classes, even when certain features are not as useful on their own. We will discuss this type of work in future blog posts, but we will show a simple way to predict the target classes from the data using logistic regression, and a naïve bayes classifier.\n\n\n\nLogistic regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes, like true or false. However, it can also handle multi-class classification problems by using techniques like one-vs-all (OvA) or softmax regression. “One-vs-all” is exactly what it sounds like; we train a binary classifier for each class, to just predict if a given data point belongs to that class or any of the others.\nLogistic regression is based on the idea of modeling the probability of an observation belonging to a particular class. It essentially predicts the probability of an observation belonging to a specific category or class. We use the sigmoid function to map the linear combination of input features to a probability score between 0 and 1, because the sigmoid function is bounded by 0 and 1, and conveniently approaches 1 as the input approaches infinity, and approaches 0 as the input approaches negative infinity.\nLogistic regression works by calculating a linear combination of the input features, then applying the sigmoid function to this value to get the probability of the observation belonging to a particular class. We can define a numerical boundary to classify probability scores as indicating one class or the other.\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nX = iris_data.data  # Feature variables\ny = iris_data.target  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create model and fit with data\nlog_reg_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg_model.fit(X_train, y_train)\n\n# Predict target values of test data using the trained model\ny_pred = log_reg_model.predict(X_test)\n\n# Examine performance of model\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 17  0]\n [ 0  0 17]]\n\n\nThe confusion matrix is a common way to display classification results. It shows the number of true positives, false positives, true negatives, and false negatives. In this case we obtained a 3 by 3 grid due to the 3 target classes. The diagonal of the matrix shows the number of true negatives, while the off-diagonal elements show the number of incorrect predictions, specifically what class was correct and what class it was predicted as.\nIn this case with a fairly straightforward dataset, you can see that our model had false positives on our test data.\nLets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n\n\n\n\n\n\n\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes’ theorem with strong (naïve) independence assumptions. It will allow us to classify the data into predicted target classes, based on the probability of the data belonging to each class, judged by the features. Naïve Bayes classifiers have found popularity for text classification, for example in dealing with spam detection.\nIt considers the probability of an observation belonging to a particular class, given the features of the observation, and classifies the observation as the class with the highest probability. Lets use the same iris dataset as before, and see how well the naïve bayes classifier does.\n\nfrom sklearn.naive_bayes import GaussianNB\n# Initialize the Naive Bayes classifier (GaussianNB is used for continuous data), and fit to training data\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = classifier.predict(X_test)\n\n# Examine performance of classifier\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nclass_report = sklearn.metrics.classification_report(y_test, y_pred, target_names=iris_data.target_names)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\nprint(\"Classification Report:\")\nprint(class_report)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 16  1]\n [ 0  1 16]]\nClassification Report:\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        19\n  versicolor       0.94      0.94      0.94        17\n   virginica       0.94      0.94      0.94        17\n\n    accuracy                           0.96        53\n   macro avg       0.96      0.96      0.96        53\nweighted avg       0.96      0.96      0.96        53\n\n\n\nAs you can see, the naïve bayes classifier performed fairly well on this dataset, although not as well as the logistic regression model. Finally, lets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()"
  },
  {
    "objectID": "posts/prob_theory_rv/index.html#introduction",
    "href": "posts/prob_theory_rv/index.html#introduction",
    "title": "Probability Theory & Random Variables",
    "section": "",
    "text": "Probability theory is a fundamental component of machine learning, as it covers the study of uncertainty and randomness. It entails the work of making predictions in the absence of complete information. In this blog post we will walk through some concepts of probability theory, show how probability theory and random variables will be important moving forward in our study of machine learning. In particular, we will discuss logistic regression and naive bayes classifiers.\n\n\nOne key aspect of probability theory is the probability distribution, which is essentially a mathematical function that provides the probabilities of occurrence of different possible outcomes in a given scenario. For instance, if we were to flip a coin, the probability distribution would be 50% heads and 50% tails. To get started, we will generate two sets of random data, to demonstrate the uniform and normal probability distributions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data from two distributions\ndata_unif = np.random.uniform(0, 1, 800)  # Uniform\ndata_norm = np.random.normal(0, 1, 800)  # Normal\n\nplt.figure(figsize=(6, 8))  # Create figure with two subplots\n\n# Uniform Distribution\nplt.subplot(2, 1, 1)\nplt.hist(data_unif, bins=20, density=True, alpha=0.6, color='green', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Uniform Probability Distribution')\n\n# Normal Distribution\nplt.subplot(2, 1, 2)\nplt.hist(data_norm, bins=20, density=True, alpha=0.6, color='purple', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Normal Probability Distribution')\n\nplt.tight_layout()  # Avoid overlapping graphs\nplt.show()  # Display\n\n\n\n\nAs you can see above, the uniform distribution appears as a rough rectangle, while the normal distribution appears as a bell curve. These are two common probability distributions, where outcomes are all equally likely, and where outcomes are more likely to be near the mean, respectively. Next lets apply some probability theory to a more practical set of data.\n\n\n\nFirst we will load the iris dataset from scikit-learn, and examine its features.\n\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris_data = load_iris()\n# Display some basic information about the dataset\nprint(\"Iris Dataset:\")\nprint(f\"{len(iris_data.target_names)} Target names: {', '.join(iris_data.target_names)}\")\nprint(f\"{len(iris_data.feature_names)} Feature names: {', '.join(iris_data.feature_names)}\")\nprint(f\"Data shape: {iris_data.data.shape}\")\n\nIris Dataset:\n3 Target names: setosa, versicolor, virginica\n4 Feature names: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)\nData shape: (150, 4)\n\n\n\nimport pandas as pd\n# Display first 5 rows of data\ndf = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\ndf[\"target\"] = iris_data.target\n# Iterate over data frame and replace target values with target names\nfor target, target_name in enumerate(iris_data.target_names):\n    df[\"target\"] = df[\"target\"].replace(target, target_name)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAs shown above, this dataset contains 150 observations, with 4 features, and 3 target classes. By loading the dataset into a pandas dataframe, we can easily visualize the data and its features.\nFirst, lets visualize the data with a kernel density estimate (KDE) plot, which allows us to visualize the distribution of observations in a dataset. The KDE plot is a smoothed version of the histogram, which shows the probability density function (PDF) of the data.\n\nimport seaborn as sns\n# KDE plot of data \nsns.displot(df, x=\"sepal length (cm)\", kde=True, hue=\"target\", stat=\"density\")\n\n\n\n\nAbove we can see the KDE plot of the sepal length feature, with the target classes colored. The “stat” argument, in this case set to “density”, means that the KDE plot will be normalized such that the area under the curve is equal to 1. This graph allows us to understand the probability distribution of our data in terms of the target classes. We can see that the setosa class has a much smaller sepal length than the other two classes.\n\nsns.displot(df, x=\"sepal width (cm)\", kde=True, hue=\"target\", stat=\"count\")\n\n\n\n\nWith this second graph, we can see that the sepal width feature is not as useful for distinguishing between the target classes, as the distributions are very similar. However, the setosa class does appear to have a slightly larger sepal width than the other two, and the versicolor class appears to have a slightly smaller sepal width than the other two. By accounting for all of the features, we can create a more accurate model for predicting the target classes, even when certain features are not as useful on their own. We will discuss this type of work in future blog posts, but we will show a simple way to predict the target classes from the data using logistic regression, and a naïve bayes classifier.\n\n\n\nLogistic regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes, like true or false. However, it can also handle multi-class classification problems by using techniques like one-vs-all (OvA) or softmax regression. “One-vs-all” is exactly what it sounds like; we train a binary classifier for each class, to just predict if a given data point belongs to that class or any of the others.\nLogistic regression is based on the idea of modeling the probability of an observation belonging to a particular class. It essentially predicts the probability of an observation belonging to a specific category or class. We use the sigmoid function to map the linear combination of input features to a probability score between 0 and 1, because the sigmoid function is bounded by 0 and 1, and conveniently approaches 1 as the input approaches infinity, and approaches 0 as the input approaches negative infinity.\nLogistic regression works by calculating a linear combination of the input features, then applying the sigmoid function to this value to get the probability of the observation belonging to a particular class. We can define a numerical boundary to classify probability scores as indicating one class or the other.\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nX = iris_data.data  # Feature variables\ny = iris_data.target  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create model and fit with data\nlog_reg_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg_model.fit(X_train, y_train)\n\n# Predict target values of test data using the trained model\ny_pred = log_reg_model.predict(X_test)\n\n# Examine performance of model\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 17  0]\n [ 0  0 17]]\n\n\nThe confusion matrix is a common way to display classification results. It shows the number of true positives, false positives, true negatives, and false negatives. In this case we obtained a 3 by 3 grid due to the 3 target classes. The diagonal of the matrix shows the number of true negatives, while the off-diagonal elements show the number of incorrect predictions, specifically what class was correct and what class it was predicted as.\nIn this case with a fairly straightforward dataset, you can see that our model had false positives on our test data.\nLets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n\n\n\n\n\n\n\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes’ theorem with strong (naïve) independence assumptions. It will allow us to classify the data into predicted target classes, based on the probability of the data belonging to each class, judged by the features. Naïve Bayes classifiers have found popularity for text classification, for example in dealing with spam detection.\nIt considers the probability of an observation belonging to a particular class, given the features of the observation, and classifies the observation as the class with the highest probability. Lets use the same iris dataset as before, and see how well the naïve bayes classifier does.\n\nfrom sklearn.naive_bayes import GaussianNB\n# Initialize the Naive Bayes classifier (GaussianNB is used for continuous data), and fit to training data\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = classifier.predict(X_test)\n\n# Examine performance of classifier\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nclass_report = sklearn.metrics.classification_report(y_test, y_pred, target_names=iris_data.target_names)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\nprint(\"Classification Report:\")\nprint(class_report)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 16  1]\n [ 0  1 16]]\nClassification Report:\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        19\n  versicolor       0.94      0.94      0.94        17\n   virginica       0.94      0.94      0.94        17\n\n    accuracy                           0.96        53\n   macro avg       0.96      0.96      0.96        53\nweighted avg       0.96      0.96      0.96        53\n\n\n\nAs you can see, the naïve bayes classifier performed fairly well on this dataset, although not as well as the logistic regression model. Finally, lets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "TODO"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nWelcome to this blog! I am currently a student in CS5805 Machine Learning at Virginia Tech. This website serves to illustrate concepts from the course in a series of five blogs. Each blog post contains code and graphs used to demonstrate a machine learning concept."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805 Blog",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\nOutlier detection\n\n\ncode\n\n\n\n\nThis is a blog post demonstrating anomaly and outlier detection.\n\n\n\n\n\n\nNov 25, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nClassification\n\n\ncode\n\n\n\n\nThis is a blog post demonstrating the concept of classification in machine learning.\n\n\n\n\n\n\nNov 24, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nLinear & Nonlinear regression\n\n\n\n\n\n\n\nRegression\n\n\ncode\n\n\n\n\nThis is a blog post demonstrating linear and nonlinear regression.\n\n\n\n\n\n\nNov 23, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nClustering\n\n\ncode\n\n\n\n\nThis blog post demonstrates the concept of clustering in machine learning.\n\n\n\n\n\n\nNov 22, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory & Random Variables\n\n\n\n\n\n\n\nProbability theory\n\n\nRandom variables\n\n\ncode\n\n\n\n\nThis blog post demonstrates concepts from probability theory and random variables.\n\n\n\n\n\n\nNov 21, 2023\n\n\nJonathan West\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anomaly_outlier_detection/index.html",
    "href": "posts/anomaly_outlier_detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "This blog post intends to demonstrate anomaly/outlier detection in machine learning. We will be examining the California housing dataset from scikit learn to explore the concept of anomaly and outlier detection. We will walk through the entire process from loading the dataset to identifying and visualizing outliers in the data.\n\nLoad and summarize the data\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California housing dataset\nhousing_data = fetch_california_housing()\n# for key in housing_data.keys():\n    # print(key)\nprint(\"Target variable:\")\nprint(housing_data.target_names)\nprint(\"Features:\")\nprint(housing_data.feature_names)\n# print(housing_data.DESCR)\n# print(housing_data.data.shape)\n\nTarget variable:\n['MedHouseVal']\nFeatures:\n['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n\n\nThis dataset contains housing data from California with the target variable being the median house value in hundreds of thousands of dollars. There are eight features in this dataset.\nNext, lets make a scatter plot to quickly display the data we are working with.\n\n# Extract the features and target variable\nX = housing_data.data  # Features\ny = housing_data.target  # Target variable\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X[:, 0], y, s=10, c='b', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Target')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html",
    "href": "posts/linear_nonlin_regression/index.html",
    "title": "Linear & Nonlinear regression",
    "section": "",
    "text": "hello\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/prob_theory_rv/index.html#referencese",
    "href": "posts/prob_theory_rv/index.html#referencese",
    "title": "Probability Theory & Random Variables",
    "section": "Referencese",
    "text": "Referencese\n\nhttps://numpy.org/doc/stable/reference/random/generator.html\nhttps://matplotlib.org/stable/gallery/statistics/hist.html\nhttps://seaborn.pydata.org/generated/seaborn.kdeplot.html\nhttps://www.datacamp.com/tutorial/understanding-logistic-regression-python\nhttps://maptv.github.io/blog/prob/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
  },
  {
    "objectID": "posts/clustering/index.html#references",
    "href": "posts/clustering/index.html#references",
    "title": "Clustering",
    "section": "References",
    "text": "References\n\nhttps://thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\nhttps://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/\nhttps://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html\nhttps://github.com/ageron/handson-ml3"
  },
  {
    "objectID": "posts/prob_theory_rv/index.html#references",
    "href": "posts/prob_theory_rv/index.html#references",
    "title": "Probability Theory & Random Variables",
    "section": "References",
    "text": "References\n\nhttps://numpy.org/doc/stable/reference/random/generator.html\nhttps://matplotlib.org/stable/gallery/statistics/hist.html\nhttps://seaborn.pydata.org/generated/seaborn.kdeplot.html\nhttps://www.datacamp.com/tutorial/understanding-logistic-regression-python\nhttps://maptv.github.io/blog/prob/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
  },
  {
    "objectID": "posts/clustering/index.html#k-means",
    "href": "posts/clustering/index.html#k-means",
    "title": "Clustering",
    "section": "K-Means",
    "text": "K-Means\nK-Means clustering is a clustering algorithm which aims to partition “n” observations into “k” different groups. As the name suggests, the grouping of each observation is determined by mean of the cluster. Essentially, the algorithm finds the mean of each cluster and assigns each observation to the cluster with the nearest mean, repeatedly recomputing the means until the clusters no longer change.\nLets exemplify this process with a fun example. We are going to use the K-Means algorithm to group the colors of an image into clusters. First, lets load an image.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n \n# Read in the image, convert to RGB\nimage = cv2.imread('landscape.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Reshaping the image into a 2D array of pixels and 3 float values (RGB)\npixels = image.reshape((-1,3))\npixels = np.float32(pixels)\n\n# Display image\nplt.imshow(image)\n\n&lt;matplotlib.image.AxesImage at 0x7f107eaaed10&gt;\n\n\n\n\n\nThis image is a matrix of pixels, where the pixels are represented by values that give their color based on the values of Red, Green, and Blue. Therefore each pixel can be considered as a point in a 3D color space, and we can use the K-Means algorithm to group the pixels in this space. To be specific, each pixel will be grouped with the cluster whose mean is closest to the pixel’s color, and these means will be updated until the clusters no longer change.\nThen, we can display the image with each pixel represented by the mean color of its cluster.\n\n# run k-means with random initial centers, k=3 clusters, stopping at 100 iterations or an epsilon value of 90%\nk = 6\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.9)\nretval, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n \n# convert data into 8-bit values, and reshape to original image dimensions\ncenters = np.uint8(centers)\nsegmented_data = centers[labels.flatten()]\nsegmented_image = segmented_data.reshape((image.shape))\n\n# Display modified image \nplt.imshow(segmented_image)\n\n&lt;matplotlib.image.AxesImage at 0x7f107c19f850&gt;\n\n\n\n\n\nHere we can see the image after having run the K-Means algorithm with K=6 clusters. You can probably think of some use cases for this, such as image compression, a stylistic effect, or simplifying an image for computer vision work or other analysis.\nTo further illustrate how k-means works, lets manually implement the algorithm on this data. Essentially we will be doing the same thing that the above code does: we initialize “k” different cluster centroids, assign each point to the cluster with the closest centroid, and then update the centroids to be the mean of the points in each cluster. We will keep repeating this process until the centroids no longer change between iterations. We start by initializing the centroids to random points in the color space. In practice we may want to ensure that the centroids are initialized to be far apart from each other or randomly sample existing colors in the image, but for simplicity we will just initialize them randomly. We also create a labels array to keep track of which cluster each pixel belongs to.\n\nimport random\nk = 4\n# Random number between 0 and 255:\nnum = random.randint(0, 255)\nmeans = np.array([[random.randint(0, 255) for _ in range(3)] for _ in range(k)])\nprint(means)\nlabels = np.zeros(pixels.shape[0], dtype=np.uint8)\nprint(pixels.shape)\nprint(labels.shape)\n\n[[ 63 255 113]\n [ 57 154  88]\n [238   0 179]\n [ 29 141 112]]\n(110880, 3)\n(110880,)\n\n\nNext, let’s make a quick helper method that will reobtain the image from the pixels array and current cluster means.\n\ndef get_image(pixels, means):\n    new_pixels = np.zeros(pixels.shape, dtype=np.uint8)\n    for i, px in enumerate(pixels):\n        new_pixels[i] = means[labels[i]]\n    new_image = new_pixels.reshape((image.shape))\n    return new_image\n\ninit_image = get_image(pixels, means)  # image with initial random means\n\nNow, we will implement the main program loop. For the sake of simplicity we will simply run 8 iterations, but in practice we would want to run until the centroids no longer change, or a certain epsilon value is reached.\n\n# Iterate over all pixels and assign them to the closest cluster\nidx = 0\nwhile idx &lt; 8:\n    # Iterate over all pixels and update their cluster assignments\n    for i, px in enumerate(pixels):\n        # Compute euclidian distance (the l2 norm) to each cluster mean\n        distances = [np.linalg.norm(px - mean) for mean in means]\n        # New cluster is the one with the smallest distance\n        cluster = np.argmin(distances)\n        labels[i] = cluster\n    if idx == 0:\n        init_image = get_image(pixels, means)\n    elif idx == 1:\n        image_1_iter = get_image(pixels, means)\n    if idx == 3:\n        image_3_iter = get_image(pixels, means)\n    # Update cluster means\n    # find average of all pixels from \"pixels\" where the label corresponds to the current cluster mean\n    old_means = np.copy(means)\n    for i in range(k):\n        means[i] = np.mean(pixels[labels == i], axis=0)\n    # Compute difference between current and previous cluster means\n    idx += 1\n\nprint(means)\n\n[[212 184 161]\n [108  98  66]\n [153 140 118]\n [ 51  61  50]]\n\n\nFinally, let’s display the image results. We can display the image with each pixel represented by the mean color of its cluster, and show how the means changed over 0, 10, 50 and 100 iterations.\n\nfinal_image = get_image(pixels, means)\nfig = plt.figure(figsize=(8, 8))\ncolumns = 2\nrows = 2\nfig.add_subplot(rows, columns, 1, title=\"Initial Clusters\")\nplt.imshow(init_image)\nfig.add_subplot(rows, columns, 2, title=\"After 1 Iteration\")\nplt.imshow(image_1_iter)\nfig.add_subplot(rows, columns, 3, title=\"After 3 Iterations\")\nplt.imshow(image_3_iter)\nfig.add_subplot(rows, columns, 4, title=\"Final Clusters\")\nplt.imshow(final_image)\nplt.show()\n\n\n\n\nNow let’s consider a more practical dataset for clustering. We are going to look at the scikit learn wine dataset, and work to cluster the wine samples into groups based on their features.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data\ny_true = wine.target  # True class labels\n\nprint(\"Wine Dataset:\")\nprint(f\"{len(wine.target_names)} Target names: {', '.join(wine.target_names)}\")\nprint(f\"{len(wine.feature_names)} Feature names: {', '.join(wine.feature_names)}\")\nprint(f\"Data shape: {wine.data.shape}\")\nprint(wine.data[:5])\n\nWine Dataset:\n3 Target names: class_0, class_1, class_2\n13 Feature names: alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline\nData shape: (178, 13)\n[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n\n\nThe dataset contains 13 features, and the target class includes 3 different types of wine. Note that because this data is not 2-dimensional, we will need to use a dimensionality reduction technique to visualize the clusters in a 2D plot. Specifically, we will use PCA, or principle component analysis, to reduce the dimensionality of the data to 2D.\n\n# Apply PCA for dimensionality reduction, so we can visualize data with 2D graph\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Perform k-means clustering\nn_clusters = 3  # You can change the number of clusters as needed\nkmeans = KMeans(n_clusters=n_clusters, random_state=0)\nkmeans.fit(X)\n\n# Get cluster labels and cluster centers\nlabels = kmeans.labels_\ncenters = kmeans.cluster_centers_\n\nplt.figure(figsize=(8, 6))\n# Plot the data points colored by the cluster labels\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\nplt.title('K-Means Clustering Results')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n/home/jrgw/CS5805_ML/blogposts/blogwork/jrgwblog/venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nAs we did before, we ran k-means clustering on the data, this time with k=3 clusters. To plot the results we used principle component analysis (PCA) to reduce the dimensionality of the data to 2D, so we could visualize the clusters in a scatter plot.\nNext, lets plot the data with the three true classification labels they were provided with in the dataset, to compare to the unsupervised learning groups.\n\nplt.figure(figsize=(8, 6))\n# Plot the data points colored by the true class labels\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, cmap='viridis')\nplt.title('True Class Labels')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\nAs we can see from comparing the two graphs, the clusters found by the k-means algorithm are comparable to the true class labels, but not quite the same. Next, lets consider how the choice of K affects our results. In this particular case, we know that there are three true wine classes, so we would expect that the best choice of K would be 3. However in practice the number of classes based on what we are trying to accomplish may not be known, so we may need to experiment with different values of K to find the best one.\nWith the k-means algorithm, “inertia” is a measure that quantifies the compactness or tightness of the clusters. It calculates the sum of the squared distances between each data point and its corresponding centroid in its cluster. Inertia allows us to assess how well the data points are grouped into clusters, because lower inertia values indicate less distance from points to their cluster centroids, and therefore the clusters are more compact.\n\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(wine.data) for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.title(\"Elbow Curve\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\nplt.grid()\nplt.show()\n\n\n\n\nWe call this graph an “Elbow Curve”, because the optimal value of K is often the point where the inertia curve has an “elbow” or sharp turn. As you can see, as our choice of k increases, the inertia decreases because more data points allow us to have more compact clusters. This will be true all the way until we reach k=n observations, where each observation is its own cluster and the inertia is 0. However, as you may have realized, this is not a good choice of k because it does not allow us to group similar observations together. The optimal value of k is the point where the inertia curve has an “elbow” or sharp turn, because this is the point where the inertia decreases more slowly as k increases, and therefore the point where the clusters are most compact, while still meaningfully grouping the data.\nIn this case, it appears that the optimal value of k is 2 rather than the 3 true classes, because the inertia curve has a sharp turn at k=2, and then decreases more slowly as k increases. This may be due to the particular criteria of wine classes not perfectly matching the wine features that we are grouping based on, or it may be due to the fact that the wine classes are not perfectly distinct from each other. In practice, if we did not have the true class labels, then we would likely choose k=2 in this case."
  },
  {
    "objectID": "posts/clustering/index.html#dbscan",
    "href": "posts/clustering/index.html#dbscan",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN"
  }
]