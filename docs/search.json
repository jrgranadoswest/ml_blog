[
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. The process is essential for finding patterns in data and is used in a variety of fields. It is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself.\nToday we will be demonstrating use of a couple different clustering algorithms."
  },
  {
    "objectID": "posts/clustering/index.html#introduction",
    "href": "posts/clustering/index.html#introduction",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. The process is essential for finding patterns in data and is used in a variety of fields. It is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself.\nToday we will be demonstrating use of a couple different clustering algorithms."
  },
  {
    "objectID": "posts/clustering/index.html#conclusion",
    "href": "posts/clustering/index.html#conclusion",
    "title": "Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, we have jumped into the concept of clustering in machine learning, which involves using unsupervised learning to find patterns and groups in unlabeled data. We discussed k-means and how to implement it, and why it is used. By understanding the concept of inertia and its pivotal role in k-means optimization, we gained insights into how to best make use of it. Clustering is an essential technique with diverse applications, allowing us to extract meaningful patterns and structures from complex datasets, making it an essential tool in a machine learning practitioner’s toolbox. Hopefully you can use this blog as an inspiration to explore clustering further, for example by trying other clustering algorithms such as DBSCAN or Gaussian Mixture Models, or by applying clustering to your own datasets."
  },
  {
    "objectID": "posts/prob_theory_rv/index.html",
    "href": "posts/prob_theory_rv/index.html",
    "title": "Probability Theory & Random Variables",
    "section": "",
    "text": "Probability theory is a fundamental component of machine learning, as it covers the study of uncertainty and randomness. It entails the work of making predictions in the absence of complete information. In this blog post we will walk through some concepts of probability theory, show how probability theory and random variables will be important moving forward in our study of machine learning. In particular, we will discuss logistic regression and naive bayes classifiers.\n\n\nOne key aspect of probability theory is the probability distribution, which is essentially a mathematical function that provides the probabilities of occurrence of different possible outcomes in a given scenario. For instance, if we were to flip a coin, the probability distribution would be 50% heads and 50% tails. To get started, we will generate two sets of random data, to demonstrate the uniform and normal probability distributions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data from two distributions\ndata_unif = np.random.uniform(0, 1, 800)  # Uniform\ndata_norm = np.random.normal(0, 1, 800)  # Normal\n\nplt.figure(figsize=(6, 8))  # Create figure with two subplots\n\n# Uniform Distribution\nplt.subplot(2, 1, 1)\nplt.hist(data_unif, bins=20, density=True, alpha=0.6, color='green', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Uniform Probability Distribution')\n\n# Normal Distribution\nplt.subplot(2, 1, 2)\nplt.hist(data_norm, bins=20, density=True, alpha=0.6, color='purple', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Normal Probability Distribution')\n\nplt.tight_layout()  # Avoid overlapping graphs\nplt.show()  # Display\n\n\n\n\nAs you can see above, the uniform distribution appears as a rough rectangle, while the normal distribution appears as a bell curve. These are two common probability distributions, where outcomes are all equally likely, and where outcomes are more likely to be near the mean, respectively. Next lets apply some probability theory to a more practical set of data.\n\n\n\nFirst we will load the iris dataset from scikit-learn, and examine its features.\n\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris_data = load_iris()\n# Display some basic information about the dataset\nprint(\"Iris Dataset:\")\nprint(f\"{len(iris_data.target_names)} Target names: {', '.join(iris_data.target_names)}\")\nprint(f\"{len(iris_data.feature_names)} Feature names: {', '.join(iris_data.feature_names)}\")\nprint(f\"Data shape: {iris_data.data.shape}\")\n\nIris Dataset:\n3 Target names: setosa, versicolor, virginica\n4 Feature names: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)\nData shape: (150, 4)\n\n\n\nimport pandas as pd\n# Display first 5 rows of data\ndf = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\ndf[\"target\"] = iris_data.target\n# Iterate over data frame and replace target values with target names\nfor target, target_name in enumerate(iris_data.target_names):\n    df[\"target\"] = df[\"target\"].replace(target, target_name)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAs shown above, this dataset contains 150 observations, with 4 features, and 3 target classes. By loading the dataset into a pandas dataframe, we can easily visualize the data and its features.\nFirst, lets visualize the data with a kernel density estimate (KDE) plot, which allows us to visualize the distribution of observations in a dataset. The KDE plot is a smoothed version of the histogram, which shows the probability density function (PDF) of the data.\n\nimport seaborn as sns\n# KDE plot of data \nsns.displot(df, x=\"sepal length (cm)\", kde=True, hue=\"target\", stat=\"density\")\n\n\n\n\nAbove we can see the KDE plot of the sepal length feature, with the target classes colored. The “stat” argument, in this case set to “density”, means that the KDE plot will be normalized such that the area under the curve is equal to 1. This graph allows us to understand the probability distribution of our data in terms of the target classes. We can see that the setosa class has a much smaller sepal length than the other two classes.\n\nsns.displot(df, x=\"sepal width (cm)\", kde=True, hue=\"target\", stat=\"count\")\n\n\n\n\nWith this second graph, we can see that the sepal width feature is not as useful for distinguishing between the target classes, as the distributions are very similar. However, the setosa class does appear to have a slightly larger sepal width than the other two, and the versicolor class appears to have a slightly smaller sepal width than the other two. By accounting for all of the features, we can create a more accurate model for predicting the target classes, even when certain features are not as useful on their own. We will discuss this type of work in future blog posts, but we will show a simple way to predict the target classes from the data using logistic regression, and a naïve bayes classifier.\n\n\n\nLogistic regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes, like true or false. However, it can also handle multi-class classification problems by using techniques like one-vs-all (OvA) or softmax regression. “One-vs-all” is exactly what it sounds like; we train a binary classifier for each class, to just predict if a given data point belongs to that class or any of the others.\nLogistic regression is based on the idea of modeling the probability of an observation belonging to a particular class. It essentially predicts the probability of an observation belonging to a specific category or class. We use the sigmoid function to map the linear combination of input features to a probability score between 0 and 1, because the sigmoid function is bounded by 0 and 1, and conveniently approaches 1 as the input approaches infinity, and approaches 0 as the input approaches negative infinity.\nLogistic regression works by calculating a linear combination of the input features, then applying the sigmoid function to this value to get the probability of the observation belonging to a particular class. We can define a numerical boundary to classify probability scores as indicating one class or the other.\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nX = iris_data.data  # Feature variables\ny = iris_data.target  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create model and fit with data\nlog_reg_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg_model.fit(X_train, y_train)\n\n# Predict target values of test data using the trained model\ny_pred = log_reg_model.predict(X_test)\n\n# Examine performance of model\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 17  0]\n [ 0  0 17]]\n\n\nThe confusion matrix is a common way to display classification results. It shows the number of true positives, false positives, true negatives, and false negatives. In this case we obtained a 3 by 3 grid due to the 3 target classes. The diagonal of the matrix shows the number of true negatives, while the off-diagonal elements show the number of incorrect predictions, specifically what class was correct and what class it was predicted as.\nIn this case with a fairly straightforward dataset, you can see that our model had false positives on our test data.\nLets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n\n\n\n\n\n\n\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes’ theorem with strong (naïve) independence assumptions. It will allow us to classify the data into predicted target classes, based on the probability of the data belonging to each class, judged by the features. Naïve Bayes classifiers have found popularity for text classification, for example in dealing with spam detection.\nIt considers the probability of an observation belonging to a particular class, given the features of the observation, and classifies the observation as the class with the highest probability. Lets use the same iris dataset as before, and see how well the naïve bayes classifier does.\n\nfrom sklearn.naive_bayes import GaussianNB\n# Initialize the Naive Bayes classifier (GaussianNB is used for continuous data), and fit to training data\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = classifier.predict(X_test)\n\n# Examine performance of classifier\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nclass_report = sklearn.metrics.classification_report(y_test, y_pred, target_names=iris_data.target_names)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\nprint(\"Classification Report:\")\nprint(class_report)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 16  1]\n [ 0  1 16]]\nClassification Report:\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        19\n  versicolor       0.94      0.94      0.94        17\n   virginica       0.94      0.94      0.94        17\n\n    accuracy                           0.96        53\n   macro avg       0.96      0.96      0.96        53\nweighted avg       0.96      0.96      0.96        53\n\n\n\nAs you can see, the naïve bayes classifier performed fairly well on this dataset, although not as well as the logistic regression model. Finally, lets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()"
  },
  {
    "objectID": "posts/prob_theory_rv/index.html#introduction",
    "href": "posts/prob_theory_rv/index.html#introduction",
    "title": "Probability Theory & Random Variables",
    "section": "",
    "text": "Probability theory is a fundamental component of machine learning, as it covers the study of uncertainty and randomness. It entails the work of making predictions in the absence of complete information. In this blog post we will walk through some concepts of probability theory, show how probability theory and random variables will be important moving forward in our study of machine learning. In particular, we will discuss logistic regression and naive bayes classifiers.\n\n\nOne key aspect of probability theory is the probability distribution, which is essentially a mathematical function that provides the probabilities of occurrence of different possible outcomes in a given scenario. For instance, if we were to flip a coin, the probability distribution would be 50% heads and 50% tails. To get started, we will generate two sets of random data, to demonstrate the uniform and normal probability distributions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data from two distributions\ndata_unif = np.random.uniform(0, 1, 800)  # Uniform\ndata_norm = np.random.normal(0, 1, 800)  # Normal\n\nplt.figure(figsize=(6, 8))  # Create figure with two subplots\n\n# Uniform Distribution\nplt.subplot(2, 1, 1)\nplt.hist(data_unif, bins=20, density=True, alpha=0.6, color='green', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Uniform Probability Distribution')\n\n# Normal Distribution\nplt.subplot(2, 1, 2)\nplt.hist(data_norm, bins=20, density=True, alpha=0.6, color='purple', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Normal Probability Distribution')\n\nplt.tight_layout()  # Avoid overlapping graphs\nplt.show()  # Display\n\n\n\n\nAs you can see above, the uniform distribution appears as a rough rectangle, while the normal distribution appears as a bell curve. These are two common probability distributions, where outcomes are all equally likely, and where outcomes are more likely to be near the mean, respectively. Next lets apply some probability theory to a more practical set of data.\n\n\n\nFirst we will load the iris dataset from scikit-learn, and examine its features.\n\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris_data = load_iris()\n# Display some basic information about the dataset\nprint(\"Iris Dataset:\")\nprint(f\"{len(iris_data.target_names)} Target names: {', '.join(iris_data.target_names)}\")\nprint(f\"{len(iris_data.feature_names)} Feature names: {', '.join(iris_data.feature_names)}\")\nprint(f\"Data shape: {iris_data.data.shape}\")\n\nIris Dataset:\n3 Target names: setosa, versicolor, virginica\n4 Feature names: sepal length (cm), sepal width (cm), petal length (cm), petal width (cm)\nData shape: (150, 4)\n\n\n\nimport pandas as pd\n# Display first 5 rows of data\ndf = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\ndf[\"target\"] = iris_data.target\n# Iterate over data frame and replace target values with target names\nfor target, target_name in enumerate(iris_data.target_names):\n    df[\"target\"] = df[\"target\"].replace(target, target_name)\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nAs shown above, this dataset contains 150 observations, with 4 features, and 3 target classes. By loading the dataset into a pandas dataframe, we can easily visualize the data and its features.\nFirst, lets visualize the data with a kernel density estimate (KDE) plot, which allows us to visualize the distribution of observations in a dataset. The KDE plot is a smoothed version of the histogram, which shows the probability density function (PDF) of the data.\n\nimport seaborn as sns\n# KDE plot of data \nsns.displot(df, x=\"sepal length (cm)\", kde=True, hue=\"target\", stat=\"density\")\n\n\n\n\nAbove we can see the KDE plot of the sepal length feature, with the target classes colored. The “stat” argument, in this case set to “density”, means that the KDE plot will be normalized such that the area under the curve is equal to 1. This graph allows us to understand the probability distribution of our data in terms of the target classes. We can see that the setosa class has a much smaller sepal length than the other two classes.\n\nsns.displot(df, x=\"sepal width (cm)\", kde=True, hue=\"target\", stat=\"count\")\n\n\n\n\nWith this second graph, we can see that the sepal width feature is not as useful for distinguishing between the target classes, as the distributions are very similar. However, the setosa class does appear to have a slightly larger sepal width than the other two, and the versicolor class appears to have a slightly smaller sepal width than the other two. By accounting for all of the features, we can create a more accurate model for predicting the target classes, even when certain features are not as useful on their own. We will discuss this type of work in future blog posts, but we will show a simple way to predict the target classes from the data using logistic regression, and a naïve bayes classifier.\n\n\n\nLogistic regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes, like true or false. However, it can also handle multi-class classification problems by using techniques like one-vs-all (OvA) or softmax regression. “One-vs-all” is exactly what it sounds like; we train a binary classifier for each class, to just predict if a given data point belongs to that class or any of the others.\nLogistic regression is based on the idea of modeling the probability of an observation belonging to a particular class. It essentially predicts the probability of an observation belonging to a specific category or class. We use the sigmoid function to map the linear combination of input features to a probability score between 0 and 1, because the sigmoid function is bounded by 0 and 1, and conveniently approaches 1 as the input approaches infinity, and approaches 0 as the input approaches negative infinity.\nLogistic regression works by calculating a linear combination of the input features, then applying the sigmoid function to this value to get the probability of the observation belonging to a particular class. We can define a numerical boundary to classify probability scores as indicating one class or the other.\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nX = iris_data.data  # Feature variables\ny = iris_data.target  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create model and fit with data\nlog_reg_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg_model.fit(X_train, y_train)\n\n# Predict target values of test data using the trained model\ny_pred = log_reg_model.predict(X_test)\n\n# Examine performance of model\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 17  0]\n [ 0  0 17]]\n\n\nThe confusion matrix is a common way to display classification results. It shows the number of true positives, false positives, true negatives, and false negatives. In this case we obtained a 3 by 3 grid due to the 3 target classes. The diagonal of the matrix shows the number of true negatives, while the off-diagonal elements show the number of incorrect predictions, specifically what class was correct and what class it was predicted as.\nIn this case with a fairly straightforward dataset, you can see that our model had false positives on our test data.\nLets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n\n\n\n\n\n\n\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes’ theorem with strong (naïve) independence assumptions. It will allow us to classify the data into predicted target classes, based on the probability of the data belonging to each class, judged by the features. Naïve Bayes classifiers have found popularity for text classification, for example in dealing with spam detection.\nIt considers the probability of an observation belonging to a particular class, given the features of the observation, and classifies the observation as the class with the highest probability. Lets use the same iris dataset as before, and see how well the naïve bayes classifier does.\n\nfrom sklearn.naive_bayes import GaussianNB\n# Initialize the Naive Bayes classifier (GaussianNB is used for continuous data), and fit to training data\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = classifier.predict(X_test)\n\n# Examine performance of classifier\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nclass_report = sklearn.metrics.classification_report(y_test, y_pred, target_names=iris_data.target_names)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\nprint(\"Classification Report:\")\nprint(class_report)\n\nConfusion Matrix:\n [[19  0  0]\n [ 0 16  1]\n [ 0  1 16]]\nClassification Report:\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        19\n  versicolor       0.94      0.94      0.94        17\n   virginica       0.94      0.94      0.94        17\n\n    accuracy                           0.96        53\n   macro avg       0.96      0.96      0.96        53\nweighted avg       0.96      0.96      0.96        53\n\n\n\nAs you can see, the naïve bayes classifier performed fairly well on this dataset, although not as well as the logistic regression model. Finally, lets visualize the confusion matrix with a heatmap.\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is an important concept in machine learning. It is used to predict the class of a given data point, or in other words assign an observation to a predefined group, based on the features of that observation. In this blog post, we will jump into the concept of classification and how to carry it out on a dataset.\nIf you have followed along in other blog posts on this site, you will likely have seen the concepts of regression and clustering. Classification is similar to both of these concepts, but it differs in some major ways. The goal of regression is to predict continuous numerical values, whereas the goal of classification is to predict discrete values. However, they are both forms of supervised learning, where we have a set of labeled data to use to create our model, before applying it to new data. On the other hand, clustering is a form of unsupervised learning, where we have a set of unlabeled data, and we are trying to find patterns in the data. Like clustering, classification entails finding patterns in the data and grouping similar data points together. However, we will be using predefined labeled groups rather than trying to find the groups ourselves.\nLet’s load a dataset to conduct classification work on.\n\n# Load the penguins dataset from the seaborn library\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\nprint(f\"Number of observations: {penguins.shape[0]}\")\n\nNumber of observations: 344\n\n\n\npenguins.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nAs shown above, there are some none-numerical values in the dataset, and there are some missing values. We will need to deal with these before we can carry out classification work on the dataset. Let’s drop the rows that have missing values.\n\npenguins = penguins.dropna()\npenguins.isna().sum()\n\nspecies              0\nisland               0\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\n\nprint(f\"Number of observations: {penguins.shape[0]}\")\n\nNumber of observations: 333\n\n\nAs you can see, we have cut the number of observations from 344 to 333, and removed any rows with missing values. Now we will continue on with our classification work."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nWelcome to this blog! I am currently a student in CS5805 Machine Learning at Virginia Tech. This website serves to illustrate concepts from the course in a series of five blogs. Each blog post contains code and graphs used to demonstrate a machine learning concept."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805 Blog",
    "section": "",
    "text": "Anomaly/Outlier Detection\n\n\n\n\n\n\n\nOutlier detection\n\n\ncode\n\n\n\n\nThis blog demonstrates anomaly and outlier detection.\n\n\n\n\n\n\nNov 25, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nClassification\n\n\ncode\n\n\n\n\nThis is a blog post demonstrating the concept of classification in machine learning.\n\n\n\n\n\n\nNov 24, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nLinear & Nonlinear regression\n\n\n\n\n\n\n\nRegression\n\n\ncode\n\n\n\n\nThis is a blog post demonstrating linear and nonlinear regression.\n\n\n\n\n\n\nNov 23, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nClustering\n\n\ncode\n\n\n\n\nThis blog post demonstrates the concept of clustering in machine learning.\n\n\n\n\n\n\nNov 22, 2023\n\n\nJonathan West\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory & Random Variables\n\n\n\n\n\n\n\nProbability theory\n\n\nRandom variables\n\n\ncode\n\n\n\n\nThis blog post demonstrates concepts from probability theory and random variables.\n\n\n\n\n\n\nNov 21, 2023\n\n\nJonathan West\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anomaly_outlier_detection/index.html",
    "href": "posts/anomaly_outlier_detection/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "This blog post intends to demonstrate anomaly/outlier detection in machine learning. We will be examining the California housing dataset from scikit learn to explore the concept of anomaly and outlier detection. We will walk through the entire process from loading the dataset to identifying and visualizing outliers in the data.\n\nLoad and summarize the data\n\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California housing dataset\nhousing_data = fetch_california_housing()\n# for key in housing_data.keys():\n    # print(key)\nprint(\"Target variable:\")\nprint(housing_data.target_names)\nprint(\"Features:\")\nprint(housing_data.feature_names)\n# print(housing_data.DESCR)\n# print(housing_data.data.shape)\n\nTarget variable:\n['MedHouseVal']\nFeatures:\n['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n\n\nThis dataset contains housing data from California with the target variable being the median house value in hundreds of thousands of dollars. There are eight features in this dataset.\nNext, lets make a scatter plot to quickly display the data we are working with.\n\n# Extract the features and target variable\nX = housing_data.data  # Features\ny = housing_data.target  # Target variable\n\n# Plotting the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X[:, 0], y, s=10, c='b', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Target')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html",
    "href": "posts/linear_nonlin_regression/index.html",
    "title": "Linear & Nonlinear regression",
    "section": "",
    "text": "Linear and nonlinear regression are important foundational techniques in machine learning and statistics used to model the relationship between independent and dependent variables in data. Linear regression is an approach to model the relationship between a development variable and one or more explanatory variables by fitting a linear equation to observed data. Nonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. Nonlinear regression allows for more complex relationships between variables, and is essential for identifying and modeling patterns in the data that are nonlinear.\nIn today’s blog post, we will be exploring linear and nonlinear regression using Python, with the scikit-learn California housing dataset."
  },
  {
    "objectID": "posts/prob_theory_rv/index.html#referencese",
    "href": "posts/prob_theory_rv/index.html#referencese",
    "title": "Probability Theory & Random Variables",
    "section": "Referencese",
    "text": "Referencese\n\nhttps://numpy.org/doc/stable/reference/random/generator.html\nhttps://matplotlib.org/stable/gallery/statistics/hist.html\nhttps://seaborn.pydata.org/generated/seaborn.kdeplot.html\nhttps://www.datacamp.com/tutorial/understanding-logistic-regression-python\nhttps://maptv.github.io/blog/prob/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
  },
  {
    "objectID": "posts/clustering/index.html#references",
    "href": "posts/clustering/index.html#references",
    "title": "Clustering",
    "section": "References",
    "text": "References\n\nhttps://thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\nhttps://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/\nhttps://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\nhttps://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html\nhttps://github.com/ageron/handson-ml3"
  },
  {
    "objectID": "posts/prob_theory_rv/index.html#references",
    "href": "posts/prob_theory_rv/index.html#references",
    "title": "Probability Theory & Random Variables",
    "section": "References",
    "text": "References\n\nhttps://numpy.org/doc/stable/reference/random/generator.html\nhttps://matplotlib.org/stable/gallery/statistics/hist.html\nhttps://seaborn.pydata.org/generated/seaborn.kdeplot.html\nhttps://www.datacamp.com/tutorial/understanding-logistic-regression-python\nhttps://maptv.github.io/blog/prob/\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
  },
  {
    "objectID": "posts/clustering/index.html#k-means",
    "href": "posts/clustering/index.html#k-means",
    "title": "Clustering",
    "section": "K-Means",
    "text": "K-Means\nK-Means clustering is a clustering algorithm which aims to partition “n” observations into “k” different groups. As the name suggests, the grouping of each observation is determined by mean of the cluster. Essentially, the algorithm finds the mean of each cluster and assigns each observation to the cluster with the nearest mean, repeatedly recomputing the means until the clusters no longer change.\nLets exemplify this process with a fun example. We are going to use the K-Means algorithm to group the colors of an image into clusters. First, lets load an image.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n \n# Read in the image, convert to RGB\nimage = cv2.imread('landscape.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Reshaping the image into a 2D array of pixels and 3 float values (RGB)\npixels = image.reshape((-1,3))\npixels = np.float32(pixels)\n\n# Display image\nplt.imshow(image)\n\n&lt;matplotlib.image.AxesImage at 0x7f0fc556bf90&gt;\n\n\n\n\n\nThis image is a matrix of pixels, where the pixels are represented by values that give their color based on the values of Red, Green, and Blue. Therefore each pixel can be considered as a point in a 3D color space, and we can use the K-Means algorithm to group the pixels in this space. To be specific, each pixel will be grouped with the cluster whose mean is closest to the pixel’s color, and these means will be updated until the clusters no longer change.\nThen, we can display the image with each pixel represented by the mean color of its cluster.\n\n# run k-means with random initial centers, k=3 clusters, stopping at 100 iterations or an epsilon value of 90%\nk = 6\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.9)\nretval, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n \n# convert data into 8-bit values, and reshape to original image dimensions\ncenters = np.uint8(centers)\nsegmented_data = centers[labels.flatten()]\nsegmented_image = segmented_data.reshape((image.shape))\n\n# Display modified image \nplt.imshow(segmented_image)\n\n&lt;matplotlib.image.AxesImage at 0x7f0fc4026650&gt;\n\n\n\n\n\nHere we can see the image after having run the K-Means algorithm with K=6 clusters. You can probably think of some use cases for this, such as image compression, a stylistic effect, or simplifying an image for computer vision work or other analysis.\nTo further illustrate how k-means works, lets manually implement the algorithm on this data. Essentially we will be doing the same thing that the above code does: we initialize “k” different cluster centroids, assign each point to the cluster with the closest centroid, and then update the centroids to be the mean of the points in each cluster. We will keep repeating this process until the centroids no longer change between iterations. We start by initializing the centroids to random points in the color space. In practice we may want to ensure that the centroids are initialized to be far apart from each other or randomly sample existing colors in the image, but for simplicity we will just initialize them randomly. We also create a labels array to keep track of which cluster each pixel belongs to.\n\nimport random\nk = 4\n# Random number between 0 and 255:\nnum = random.randint(0, 255)\nmeans = np.array([[random.randint(0, 255) for _ in range(3)] for _ in range(k)])\nprint(means)\nlabels = np.zeros(pixels.shape[0], dtype=np.uint8)\nprint(pixels.shape)\nprint(labels.shape)\n\n[[127  16  49]\n [ 86  68 218]\n [209  43 100]\n [203 171 224]]\n(110880, 3)\n(110880,)\n\n\nNext, let’s make a quick helper method that will reobtain the image from the pixels array and current cluster means.\n\ndef get_image(pixels, means):\n    new_pixels = np.zeros(pixels.shape, dtype=np.uint8)\n    for i, px in enumerate(pixels):\n        new_pixels[i] = means[labels[i]]\n    new_image = new_pixels.reshape((image.shape))\n    return new_image\n\ninit_image = get_image(pixels, means)  # image with initial random means\n\nNow, we will implement the main program loop. For the sake of simplicity we will simply run 8 iterations, but in practice we would want to run until the centroids no longer change, or a certain epsilon value is reached.\n\n# Iterate over all pixels and assign them to the closest cluster\nidx = 0\nwhile idx &lt; 8:\n    # Iterate over all pixels and update their cluster assignments\n    for i, px in enumerate(pixels):\n        # Compute euclidian distance (the l2 norm) to each cluster mean\n        distances = [np.linalg.norm(px - mean) for mean in means]\n        # New cluster is the one with the smallest distance\n        cluster = np.argmin(distances)\n        labels[i] = cluster\n    if idx == 0:\n        init_image = get_image(pixels, means)\n    elif idx == 1:\n        image_1_iter = get_image(pixels, means)\n    if idx == 3:\n        image_3_iter = get_image(pixels, means)\n    # Update cluster means\n    # find average of all pixels from \"pixels\" where the label corresponds to the current cluster mean\n    old_means = np.copy(means)\n    for i in range(k):\n        means[i] = np.mean(pixels[labels == i], axis=0)\n    # Compute difference between current and previous cluster means\n    idx += 1\n\nprint(means)\n\n[[ 59  62  45]\n [ 81 111 114]\n [130 107  63]\n [195 171 150]]\n\n\nFinally, let’s display the image results. We can display the image with each pixel represented by the mean color of its cluster, and show how the means changed over 0, 10, 50 and 100 iterations.\n\nfinal_image = get_image(pixels, means)\nfig = plt.figure(figsize=(8, 8))\ncolumns = 2\nrows = 2\nfig.add_subplot(rows, columns, 1, title=\"Initial Clusters\")\nplt.imshow(init_image)\nfig.add_subplot(rows, columns, 2, title=\"After 1 Iteration\")\nplt.imshow(image_1_iter)\nfig.add_subplot(rows, columns, 3, title=\"After 3 Iterations\")\nplt.imshow(image_3_iter)\nfig.add_subplot(rows, columns, 4, title=\"Final Clusters\")\nplt.imshow(final_image)\nplt.show()\n\n\n\n\nNow let’s consider a more practical dataset for clustering. We are going to look at the scikit learn wine dataset, and work to cluster the wine samples into groups based on their features.\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data\ny_true = wine.target  # True class labels\n\nprint(\"Wine Dataset:\")\nprint(f\"{len(wine.target_names)} Target names: {', '.join(wine.target_names)}\")\nprint(f\"{len(wine.feature_names)} Feature names: {', '.join(wine.feature_names)}\")\nprint(f\"Data shape: {wine.data.shape}\")\nprint(wine.data[:5])\n\nWine Dataset:\n3 Target names: class_0, class_1, class_2\n13 Feature names: alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline\nData shape: (178, 13)\n[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n\n\nThe dataset contains 13 features, and the target class includes 3 different types of wine. Note that because this data is not 2-dimensional, we will need to use a dimensionality reduction technique to visualize the clusters in a 2D plot. Specifically, we will use PCA, or principle component analysis, to reduce the dimensionality of the data to 2D.\n\n# Apply PCA for dimensionality reduction, so we can visualize data with 2D graph\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Perform k-means clustering\nn_clusters = 3  # You can change the number of clusters as needed\nkmeans = KMeans(n_clusters=n_clusters, random_state=0)\nkmeans.fit(X)\n\n# Get cluster labels and cluster centers\nlabels = kmeans.labels_\ncenters = kmeans.cluster_centers_\n\nplt.figure(figsize=(8, 6))\n# Plot the data points colored by the cluster labels\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\nplt.title('K-Means Clustering Results')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n/home/jrgw/CS5805_ML/blogposts/blogwork/jrgwblog/venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\nAs we did before, we ran k-means clustering on the data, this time with k=3 clusters. To plot the results we used principle component analysis (PCA) to reduce the dimensionality of the data to 2D, so we could visualize the clusters in a scatter plot.\nNext, lets plot the data with the three true classification labels they were provided with in the dataset, to compare to the unsupervised learning groups.\n\nplt.figure(figsize=(8, 6))\n# Plot the data points colored by the true class labels\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, cmap='viridis')\nplt.title('True Class Labels')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\nAs we can see from comparing the two graphs, the clusters found by the k-means algorithm are comparable to the true class labels, but not quite the same. Next, lets consider how the choice of K affects our results. In this particular case, we know that there are three true wine classes, so we would expect that the best choice of K would be 3. However in practice the number of classes based on what we are trying to accomplish may not be known, so we may need to experiment with different values of K to find the best one.\nWith the k-means algorithm, “inertia” is a measure that quantifies the compactness or tightness of the clusters. It calculates the sum of the squared distances between each data point and its corresponding centroid in its cluster. Inertia allows us to assess how well the data points are grouped into clusters, because lower inertia values indicate less distance from points to their cluster centroids, and therefore the clusters are more compact.\n\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(wine.data) for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.title(\"Elbow Curve\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\nplt.grid()\nplt.show()\n\n\n\n\nWe call this graph an “Elbow Curve”, because the optimal value of K is often the point where the inertia curve has an “elbow” or sharp turn. As you can see, as our choice of k increases, the inertia decreases because more data points allow us to have more compact clusters. This will be true all the way until we reach k=n observations, where each observation is its own cluster and the inertia is 0. However, as you may have realized, this is not a good choice of k because it does not allow us to group similar observations together. The optimal value of k is the point where the inertia curve has an “elbow” or sharp turn, because this is the point where the inertia decreases more slowly as k increases, and therefore the point where the clusters are most compact, while still meaningfully grouping the data.\nIn this case, it appears that the optimal value of k is 2 rather than the 3 true classes, because the inertia curve has a sharp turn at k=2, and then decreases more slowly as k increases. This may be due to the particular criteria of wine classes not perfectly matching the wine features that we are grouping based on, or it may be due to the fact that the wine classes are not perfectly distinct from each other. In practice, if we did not have the true class labels, then we would likely choose k=2 in this case."
  },
  {
    "objectID": "posts/clustering/index.html#dbscan",
    "href": "posts/clustering/index.html#dbscan",
    "title": "Clustering",
    "section": "DBSCAN",
    "text": "DBSCAN"
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html#introduction",
    "href": "posts/linear_nonlin_regression/index.html#introduction",
    "title": "Linear & Nonlinear regression",
    "section": "",
    "text": "Linear and nonlinear regression are important foundational techniques in machine learning and statistics used to model the relationship between independent and dependent variables in data. Linear regression is an approach to model the relationship between a development variable and one or more explanatory variables by fitting a linear equation to observed data. Nonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. Nonlinear regression allows for more complex relationships between variables, and is essential for identifying and modeling patterns in the data that are nonlinear.\nIn today’s blog post, we will be exploring linear and nonlinear regression using Python, with the scikit-learn California housing dataset."
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html#references",
    "href": "posts/linear_nonlin_regression/index.html#references",
    "title": "Linear & Nonlinear regression",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html#sources-used",
    "href": "posts/linear_nonlin_regression/index.html#sources-used",
    "title": "Linear & Nonlinear regression",
    "section": "Sources used",
    "text": "Sources used\n\nhttps://realpython.com/linear-regression-in-python/\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html\nhttps://github.com/ageron/handson-ml3\nhttps://www.investopedia.com/terms/m/mlr.asp\nhttps://medium.com/machine-learning-with-python/multiple-linear-regression-implementation-in-python-2de9b303fc0c"
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html#linear-regression",
    "href": "posts/linear_nonlin_regression/index.html#linear-regression",
    "title": "Linear & Nonlinear regression",
    "section": "Linear regression",
    "text": "Linear regression\nAs mentioned above, linear regression is a linear approach to modeling the relationship between a development variable and one or more explanatory variables. We can use linear regression to predict the value of future dependent variable observations, by modeling on recorded data.\nLet’s start by conducting simple linear regression on some randomly generated data. We will use the numpy library to create a dataset randomly distributed around the line `y = 2.5x + 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample data\nnp.random.seed(42)\nX = 2.5 * np.random.randn(100) + 2  # Array of 100 values centered at 2 stddev = 3\ny = 0.5 * X + np.random.randn(100)  # Linear equation y = 0.5x, with added random noise\nplt.scatter(X, y, color='blue')\nplt.title('Simple Linear Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\nHere we have generated a dataset of 100 points, with a linear relationship y = 0.5x - 3, and random noise added to the data. Now we will conduct simple linear regression to fit a line to the data, and make predictions.\n\n# Create a linear regression model and fit it to the data\n# Find means of x & y\nx_mean = np.mean(X)\ny_mean = np.mean(y)\n\n# Calculate terms needed for the numerator and denominator of predicted slope \nnumerator = np.sum((X - x_mean) * (y - y_mean))\ndenominator = np.sum((X - x_mean) ** 2)\n\n# Calculate the linear equation coefficients\nslope = numerator / denominator\nintercept = y_mean - (slope * x_mean)\n\n# Create predicted line\ny_pred = slope * X + intercept\n\n# Plot the data points and the predicted line\nplt.scatter(X, y, color='green')\nlabel = f'y = {slope:.2f}x + {intercept:.2f}'\nplt.plot(X, y_pred, color='blue')\nplt.title('Simple Linear Regression: ' + label)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\nWe can see that the line of best fit is very close to the original line, and the predicted line is a good fit for the data. This is a simple toy example of linear regression, which is based in basic algebra and uses the least squares method to find the line of best fit. The example worked quite nicely because the data was generated with a linear relationship, with some random noise added. Now, lets use the concept of regression to predict the median house value in Californian districts, using the California housing dataset.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhd = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nhd[housing_data.target_names[0]] = housing_data.target  # Include target variable in dataframe: median house value\n\n# Display first 5 rows of data\nhd.head()\n\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\nAs usual, we will start by exploring the data. We can see that there are 8 features, and 1 target variable: median house value. Our goal is to predict the median house value based on the other features. Lets start by looking at the distribution of the target variable, median house value in hundreds of thousands of dollars.\n\n# Plot the distribution of the target variable\nsns.distplot(hd[housing_data.target_names[0]])\nplt.title('Distribution of median house value')\nplt.xlabel('Median house value')\nplt.ylabel('Frequency')\nplt.show()\n\n/tmp/ipykernel_13195/3123216598.py:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(hd[housing_data.target_names[0]])\n\n\n\n\n\nThe median house value is distributed roughly normally, with a mean of around 2.1 and a slight right skew. Now lets look at the correlation between the target variable and the other features. We will create scatterplots of the target variable against each feature, and calculate the correlation coefficient.\n\n# Assuming df is your DataFrame with the 9 numerical features\n# and \"MedHouseValue\" is the target feature\n# Select the 8 numerical features you want to plot\nfeatures_to_plot = housing_data.feature_names\n\n# Create a new figure with 8 subplots\nfig, axes = plt.subplots(4, 2, figsize=(8, 16))\nfig.suptitle(\"Scatterplots of Numerical Features vs. Median House Value\", size=14)\n\n# Flatten the 2D array of subplots into a 1D array\naxes = axes.flatten()\n\n# Loop through the selected features and create scatterplots\nfor i, feature in enumerate(features_to_plot):\n    ax = axes[i]\n    ax.scatter(hd[feature], hd[\"MedHouseVal\"], alpha=0.5)\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Median House Value (in $100,000s)\")\n\n# Adjust the layout and display\nplt.tight_layout()\nplt.show()\n\n\n\n\nAs we can see from these graphs, most of the features lack a strong linear relationship with the target variable. The strongest linear relationship is between median income and median house value, which makes sense. Higher income areas tend to have higher house prices.\nIn any case, lets use linear regression to predict the median house value based on median income, which seems to be the most promising feature.\n\nfrom sklearn.linear_model import LinearRegression\n\n# Simple linear regression\nX = hd['MedInc'].values\ny = hd['MedHouseVal'].values\n# reshape X\nX = X.reshape(-1, 1)\n\n# Create linear regression model & fit to data\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Obtain equation coefficients, & predict y values\nslope = model.coef_\nintercept = model.intercept_\ny_pred = model.predict(X)\n\n# Display our results\nplt.scatter(X, y, color='green')  # actual points\nplt.plot(X, y_pred, color='blue')  # predicted line\nplt.title('House value based on household income: ' + f'y = {slope[0]:.2f}x + {intercept:.2f}')\nplt.xlabel('Median income')\nplt.ylabel('Median house value')\nplt.show()\n\n\n\n\nAs we can see, the line of best fit roughly correlates to the data, but there are many outliers. Let’s calculate the mean squared error to see how well our model fits the data.\n\n# Compute mean squared error\nmse = np.mean((y_pred - y) ** 2)\nprint(f'Mean squared error: {mse}')\n\nMean squared error: 0.7011311502929527\n\n\nIn this case the mean squared error is relatively high, which we could have expected simply by looking at the graph. Perhaps the data is not linearly related, and we should try a nonlinear regression model instead. We can now move on to nonlinear regression."
  },
  {
    "objectID": "posts/linear_nonlin_regression/index.html#nonlinear-regression",
    "href": "posts/linear_nonlin_regression/index.html#nonlinear-regression",
    "title": "Linear & Nonlinear regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\nNonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. It is a fundamental concept in machine learning, and is used to model complex relationships between variables.\nLet’s apply nonlinear regression to the California housing dataset, and see if we can improve our predictions of median house value.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Obtain data again\nX2 = hd['MedInc'].values\ny2 = hd['MedHouseVal'].values\n\n# Reshape X\nX2 = X2.reshape(-1, 1)\n\n# Create Polynomial Features\ndegree = 2  # You can adjust the degree of the polynomial\ndef show_nonlin_regression(X, y, degree):\n    poly_features = PolynomialFeatures(degree=degree)\n    X_poly = poly_features.fit_transform(X)\n\n    # Create and fit Polynomial Regression model\n    poly_model = LinearRegression()\n    poly_model.fit(X_poly, y)\n\n    # Predict y values\n    y_pred = poly_model.predict(X_poly)\n\n    # Sort X and y_pred for better visualization\n    sort_idx = np.argsort(X[:, 0])\n    X_sorted = X[sort_idx]\n    y_pred_sorted = y_pred[sort_idx]\n\n    # Display results\n    plt.scatter(X, y, color='green', label='Actual data')\n    plt.plot(X_sorted, y_pred_sorted, color='blue', label='Polynomial Regression')\n    plt.title(f'House value based on household income (Polynomial Regression - Degree {degree})')\n    plt.xlabel('Median income')\n    plt.ylabel('Median house value')\n    plt.legend()\n    plt.show()\n    return y_pred\n\ny_pred2 = show_nonlin_regression(X2, y2, degree)\n\n\n\n\nThis time it appears the regression model fits the data slightly better, but not by much.\n\n# Compute mean squared error\nmse = np.mean((y_pred2 - y2) ** 2)\nprint(f'Mean squared error: {mse}')\n\nMean squared error: 0.695037253723973\n\n\nAs you can see, the mean squared error is slightly lower than the linear regression model, but still quite high. Let’s try a higher degree polynomial.\n\n# Obtain data again\nX3 = hd['MedInc'].values\ny3 = hd['MedHouseVal'].values\nX3 = X3.reshape(-1, 1)\ny_pred3 = show_nonlin_regression(X3, y3, 3)\n\n\n\n\n\n# Compute mean squared error\nmse = np.mean((y_pred2 - y2) ** 2)\nprint(f'Mean squared error: {mse}')\n\nMean squared error: 0.695037253723973\n\n\nYet again, the model doesn’t fit the data incredibly well, and the mean squared error is still quite high. Additionally, it appears that the model is begginning to overfit the data, as the line of best fit is beginning to curve too much to fit the data. It doesn’t make sense for very high income areas to have somewhat lower house prices, yet the model predicts this. With simple regression it is difficult to model the relationship between median income and median house value, as there are many other factors that affect house prices. The variance in house prices is likely due to other factors, such as location, house size, and age of the house. However, we have still illustrated how linear and nonlinear regression can be applied to predict the value of future dependent variable observations, by modeling on recorded data.\nLastly, let’s try to predict the median house value based on the other features, using multiple linear regression.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Prepare feature matrix and target \nX = hd.drop(columns=['MedHouseVal'])  # Use all columns except 'MedHouseVal' as features\ny = hd['MedHouseVal']\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and fit multiple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Interpret the model coefficients\ncoefficients = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': model.coef_})\nintercept = model.intercept_\nprint(\"Intercept:\", intercept)\nprint(\"Coefficients:\")\nprint(coefficients)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"R-squared (R2) Score:\", r2)\n\nIntercept: -37.02327770606409\nCoefficients:\n      Feature  Coefficient\n0      MedInc     0.448675\n1    HouseAge     0.009724\n2    AveRooms    -0.123323\n3   AveBedrms     0.783145\n4  Population    -0.000002\n5    AveOccup    -0.003526\n6    Latitude    -0.419792\n7   Longitude    -0.433708\nMean Squared Error (MSE): 0.5558915986952444\nR-squared (R2) Score: 0.5757877060324508\n\n\nAs we can see, the multiple linear regression model fits the data much better than the simple linear regression model, and the mean squared error is much lower. Incorporating multiple features allows us to more closely model the real life relationships between these variables. Now lets use a final visualization for the work we have done.\n\n# Plot actual vs predicted vals\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs. Predicted Values')\nplt.show()\n\n\n\n\nThere does appear to be a linear relationship between the actual and predicted values, following the equation y=x. This is a good sign, as it means our model is predicting the values reasonably. However we can see that the success of the regression was still limited by our choice of model, data features, and potentially outliers in the data."
  },
  {
    "objectID": "posts/classification/index.html#sources-used",
    "href": "posts/classification/index.html#sources-used",
    "title": "Classification",
    "section": "Sources used",
    "text": "Sources used\n\nhttps://github.com/ageron/handson-ml3/tree/main\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\nhttps://www.datacamp.com/tutorial/svm-classification-scikit-learn-python\nhttps://www.datacamp.com/tutorial/random-forests-classifier-python\nhttps://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/"
  },
  {
    "objectID": "posts/classification/index.html#introduction",
    "href": "posts/classification/index.html#introduction",
    "title": "Classification",
    "section": "",
    "text": "Classification is an important concept in machine learning. It is used to predict the class of a given data point, or in other words assign an observation to a predefined group, based on the features of that observation. In this blog post, we will jump into the concept of classification and how to carry it out on a dataset.\nIf you have followed along in other blog posts on this site, you will likely have seen the concepts of regression and clustering. Classification is similar to both of these concepts, but it differs in some major ways. The goal of regression is to predict continuous numerical values, whereas the goal of classification is to predict discrete values. However, they are both forms of supervised learning, where we have a set of labeled data to use to create our model, before applying it to new data. On the other hand, clustering is a form of unsupervised learning, where we have a set of unlabeled data, and we are trying to find patterns in the data. Like clustering, classification entails finding patterns in the data and grouping similar data points together. However, we will be using predefined labeled groups rather than trying to find the groups ourselves.\nLet’s load a dataset to conduct classification work on.\n\n# Load the penguins dataset from the seaborn library\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\n\nprint(f\"Number of observations: {penguins.shape[0]}\")\n\nNumber of observations: 344\n\n\n\npenguins.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nAs shown above, there are some none-numerical values in the dataset, and there are some missing values. We will need to deal with these before we can carry out classification work on the dataset. Let’s drop the rows that have missing values.\n\npenguins = penguins.dropna()\npenguins.isna().sum()\n\nspecies              0\nisland               0\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\nsex                  0\ndtype: int64\n\n\n\nprint(f\"Number of observations: {penguins.shape[0]}\")\n\nNumber of observations: 333\n\n\nAs you can see, we have cut the number of observations from 344 to 333, and removed any rows with missing values. Now we will continue on with our classification work."
  },
  {
    "objectID": "posts/classification/index.html#types-of-classification",
    "href": "posts/classification/index.html#types-of-classification",
    "title": "Classification",
    "section": "Types of classification",
    "text": "Types of classification\nThere are several different classification algorithms that we can use to classify our data. Some of the most common ones include linear classifiers, tree-based classifiers, and neural network classifiers. In this blog post we will be trying two specific algorithms: support vector machines and random forests. We will be using the scikit-learn library to carry out our classification work.\nRandom forests are a type of tree-based classifier, which use multiple decision trees to classify data, which helps improve accuracy and reduce overfitting.\nSupport vector machines (SVM) are a type of linear classifier, which find a hyperplane that best separates data into different classes while maximizing the margin between the hyperplane and the data points. SVMs are a very popular classification algorithm, and they are used in both linear and non-linear classification problems. We will start with support vector machines."
  },
  {
    "objectID": "posts/classification/index.html#svm-classification",
    "href": "posts/classification/index.html#svm-classification",
    "title": "Classification",
    "section": "SVM classification",
    "text": "SVM classification\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Replace categorical target variable with numerical values\nlabel_encoder = LabelEncoder()\npenguins[\"species_encoded\"] = label_encoder.fit_transform(penguins[\"species\"])\n\n# Obtain separate feature and target sets (X and y)\ny = penguins[\"species_encoded\"]\nX = penguins.drop([\"species\", \"species_encoded\"], axis=1)\n\nAs you can see, we have done some basic preprocessing on the dataset, including converting the target variable to numberical values. Next we need to deal with the categorical variables in the dataset. We will use label encoding to convert the sex feature into numerical values, because this can simply be done for binary or naturally ordinal variables. However, the island variable is not binary nor does it follow a naturally ordinal state, so to keep it in without creating false relationships in the data we would likely want to use one-hot encoding. One-hot encoding essentially entails creating a new binary column for each possible value of the categorical variable, and assigning a 1 or 0 to each observation depending on which value it has for that variable. However in this case we will simply drop the island variable from the dataset.\n\npenguins[\"sex_encoded\"] = label_encoder.fit_transform(penguins[\"sex\"])\nX = penguins.drop([\"species\", \"species_encoded\", \"island\", \"sex\"], axis=1)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX.head()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex_encoded\n\n\n\n\n0\n39.1\n18.7\n181.0\n3750.0\n1\n\n\n1\n39.5\n17.4\n186.0\n3800.0\n0\n\n\n2\n40.3\n18.0\n195.0\n3250.0\n0\n\n\n4\n36.7\n19.3\n193.0\n3450.0\n0\n\n\n5\n39.3\n20.6\n190.0\n3650.0\n1\n\n\n\n\n\n\n\nWe have now finished our data preprocessing work, and splitting up our training and testing sets. Now we can create our SVM classifier and fit it to the training data.\n\n# Create an SVM classifier\nsvm_classifier = SVC(kernel='linear')\n\n# Fit the classifier on the training data\nsvm_classifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = svm_classifier.predict(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 0.9850746268656716\nConfusion Matrix:\n [[30  1  0]\n [ 0 13  0]\n [ 0  0 23]]\n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.98        31\n           1       0.93      1.00      0.96        13\n           2       1.00      1.00      1.00        23\n\n    accuracy                           0.99        67\n   macro avg       0.98      0.99      0.98        67\nweighted avg       0.99      0.99      0.99        67\n\n\n\nAs we can see from the results above, our SVM classifier has an accuracy of 0.99, which is quite good. Let’s help visualize the results by creating a confusion matrix.\n\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=penguins[\"species\"].unique())\nconf_mat_display.plot()\nplt.show()\n\n\n\n\nAs we can see from the confusion matrix, our SVM classifier did a very good job of classifying the data, with only a single false positive. The model predicted a Chinstrap penguin as an Adelie penguin, but that was the only failure in the dataset.\nAlthough this worked quite well already, let’s try another classification algorithm to see how the results compare."
  },
  {
    "objectID": "posts/classification/index.html#random-forest-classification",
    "href": "posts/classification/index.html#random-forest-classification",
    "title": "Classification",
    "section": "Random forest classification",
    "text": "Random forest classification\nAs mentioned before, we are going to apply a random forest classification to the data to see how it compares to the SVM classifier. Random forest classification works by creating multiple decision trees, and then using the mode of the predictions of the individual trees as the final prediction. We already did preprocessing on the data and split it into training and testing sets, so we can simply create a new classifier and fit it to the training data.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest classifier\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Fit the classifier on the training data & make predictions\nrandom_forest_classifier.fit(X_train, y_train)\ny_pred = random_forest_classifier.predict(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(classification_report(y_test, y_pred))\n\nAccuracy: 1.0\nConfusion Matrix:\n [[31  0  0]\n [ 0 13  0]\n [ 0  0 23]]\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        31\n           1       1.00      1.00      1.00        13\n           2       1.00      1.00      1.00        23\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=penguins[\"species\"].unique())\nconf_mat_display.plot()\nplt.show()\n\n\n\n\nThis classifier also did a great job, even beating out the SVM in terms of accuracy. For this relatively simple dataset, random forest classifcation was able to achieve 100% accuracy on our testing data.\nLastly, lets create another visualization of the classification, this time visualizing the first three trees of our random forest classifier.\n\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nimport graphviz\n\nfor i in range(3):\n    tree = random_forest_classifier.estimators_[i]\n    dot_data = export_graphviz(tree,\n                               feature_names=X_train.columns,  \n                               filled=True,  \n                               max_depth=2, \n                               impurity=False, \n                               proportion=True)\n    graph = graphviz.Source(dot_data)\n    display(graph)\n\n\n\n\n\n\n\n\n\n\nThese decision trees show how for a given feature, the tree splits the data into two groups based on the value of that feature, and narrows down the possible classes for each group. By combining the results of multiple trees, random forest classification is able to achieve a more comprehensive classification than a single decision tree."
  }
]