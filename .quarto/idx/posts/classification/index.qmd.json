{"title":"Classification","markdown":{"yaml":{"title":"Classification","author":"Jonathan West","date":"2023-11-24","categories":["Classification","code"],"description":"This is a blog post demonstrating the concept of classification in machine learning.","format":{"html":{"code-tools":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nClassification is an important concept in machine learning.\nIt is used to predict the class of a given data point, or in other words assign an observation to a predefined group, based on the features of that observation.\nIn this blog post, we will jump into the concept of classification and how to carry it out on a dataset.\n\nIf you have followed along in other blog posts on this site, you will likely have seen the concepts of regression and clustering. Classification is similar to both of these concepts, but it differs in some major ways. The goal of regression is to predict continuous numerical values, whereas the goal of classification is to predict discrete values. However, they are both forms of supervised learning, where we have a set of labeled data to use to create our model, before applying it to new data.\nOn the other hand, clustering is a form of unsupervised learning, where we have a set of unlabeled data, and we are trying to find patterns in the data. Like clustering, classification entails finding patterns in the data and grouping similar data points together. However, we will be using predefined labeled groups rather than trying to find the groups ourselves.\n\nLet's load a dataset to conduct classification work on.\n\n```{python}\n# Load the penguins dataset from the seaborn library\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n```\n\n```{python}\nprint(f\"Number of observations: {penguins.shape[0]}\")\n```\n\n```{python}\npenguins.isna().sum()\n```\n\nAs shown above, there are some none-numerical values in the dataset, and there are some missing values. We will need to deal with these before we can carry out classification work on the dataset. Let's drop the rows that have missing values.\n\n```{python}\npenguins = penguins.dropna()\npenguins.isna().sum()\n```\n```{python}\nprint(f\"Number of observations: {penguins.shape[0]}\")\n```\n\nAs you can see, we have cut the number of observations from 344 to 333, and removed any rows with missing values. Now we will continue on with our classification work.\n\n## Types of classification\nThere are several different classification algorithms that we can use to classify our data. Some of the most common ones include linear classifiers, tree-based classifiers, and neural network classifiers. \nIn this blog post we will be trying two specific algorithms: support vector machines and random forests. We will be using the scikit-learn library to carry out our classification work.\n\nRandom forests are a type of tree-based classifier, which use multiple decision trees to classify data, which helps improve accuracy and reduce overfitting.\n\nSupport vector machines (SVM) are a type of linear classifier, which find a hyperplane that best separates data into different classes while maximizing the margin between the hyperplane and the data points. SVMs are a very popular classification algorithm, and they are used in both linear and non-linear classification problems. \nWe will start with support vector machines.\n\n## SVM classification\n```{python}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Replace categorical target variable with numerical values\nlabel_encoder = LabelEncoder()\npenguins[\"species_encoded\"] = label_encoder.fit_transform(penguins[\"species\"])\n\n# Obtain separate feature and target sets (X and y)\ny = penguins[\"species_encoded\"]\nX = penguins.drop([\"species\", \"species_encoded\"], axis=1)\n\n```\n\nAs you can see, we have done some basic preprocessing on the dataset, including converting the target variable to numberical values.\nNext we need to deal with the categorical variables in the dataset. We will use label encoding to convert the `sex` feature into numerical values, because this can simply be done for binary or naturally ordinal variables. \nHowever, the island variable is not binary nor does it follow a naturally ordinal state, so to keep it in without creating false relationships in the data we would likely want to use one-hot encoding.\nOne-hot encoding essentially entails creating a new binary column for each possible value of the categorical variable, and assigning a 1 or 0 to each observation depending on which value it has for that variable. \nHowever in this case we will simply drop the island variable from the dataset.\n\n```{python}\npenguins[\"sex_encoded\"] = label_encoder.fit_transform(penguins[\"sex\"])\nX = penguins.drop([\"species\", \"species_encoded\", \"island\", \"sex\"], axis=1)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX.head()\n```\n\nWe have now finished our data preprocessing work, and splitting up our training and testing sets. \nNow we can create our SVM classifier and fit it to the training data.\n\n```{python}\n# Create an SVM classifier\nsvm_classifier = SVC(kernel='linear')\n\n# Fit the classifier on the training data\nsvm_classifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = svm_classifier.predict(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(classification_report(y_test, y_pred))\n```\n\nAs we can see from the results above, our SVM classifier has an accuracy of 0.99, which is quite good.\nLet's help visualize the results by creating a confusion matrix.\n\n```{python}\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=penguins[\"species\"].unique())\nconf_mat_display.plot()\nplt.show()\n```\n\nAs we can see from the confusion matrix, our SVM classifier did a very good job of classifying the data, with only a single false positive. The model predicted a Chinstrap penguin as an Adelie penguin, but that was the only failure in the dataset. \n\nAlthough this worked quite well already, let's try another classification algorithm to see how the results compare. \n\n## Random forest classification\nAs mentioned before, we are going to apply a random forest classification to the data to see how it compares to the SVM classifier.\nRandom forest classification works by creating multiple decision trees, and then using the mode of the predictions of the individual trees as the final prediction.\nWe already did preprocessing on the data and split it into training and testing sets, so we can simply create a new classifier and fit it to the training data.\n```{python}\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest classifier\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Fit the classifier on the training data & make predictions\nrandom_forest_classifier.fit(X_train, y_train)\ny_pred = random_forest_classifier.predict(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(classification_report(y_test, y_pred))\n```\n\n```{python}\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=penguins[\"species\"].unique())\nconf_mat_display.plot()\nplt.show()\n```\n\nThis classifier also did a great job, even beating out the SVM in terms of accuracy. For this relatively simple dataset, random forest classifcation was able to achieve 100% accuracy on our testing data.\n\nLastly, lets create another visualization of the classification, this time visualizing the first three trees of our random forest classifier.\n\n```{python}\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nimport graphviz\n\nfor i in range(3):\n    tree = random_forest_classifier.estimators_[i]\n    dot_data = export_graphviz(tree,\n                               feature_names=X_train.columns,  \n                               filled=True,  \n                               max_depth=2, \n                               impurity=False, \n                               proportion=True)\n    graph = graphviz.Source(dot_data)\n    display(graph)\n```\n\nThese decision trees show how for a given feature, the tree splits the data into two groups based on the value of that feature, and narrows down the possible classes for each group. \nBy combining the results of multiple trees, random forest classification is able to achieve a more comprehensive classification than a single decision tree.\n\n# Conclusion\nIn this blog post, we delved into the concept of classification in machine learning.\nWe looked at the difference between classification and the concepts of regression and clustering, and we looked at some different types of classification. \nWe then worked on a real use case of classification algorithms. \nI hope that this blog post has been helpful in understanding classificaiton in machine learning, and encourages you to continue with your own work in this area.\n\n\n## Sources used\n1. https://github.com/ageron/handson-ml3/tree/main\n2. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n3. https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python\n4. https://www.datacamp.com/tutorial/random-forests-classifier-python\n5. https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/","srcMarkdownNoYaml":"\n\n## Introduction\nClassification is an important concept in machine learning.\nIt is used to predict the class of a given data point, or in other words assign an observation to a predefined group, based on the features of that observation.\nIn this blog post, we will jump into the concept of classification and how to carry it out on a dataset.\n\nIf you have followed along in other blog posts on this site, you will likely have seen the concepts of regression and clustering. Classification is similar to both of these concepts, but it differs in some major ways. The goal of regression is to predict continuous numerical values, whereas the goal of classification is to predict discrete values. However, they are both forms of supervised learning, where we have a set of labeled data to use to create our model, before applying it to new data.\nOn the other hand, clustering is a form of unsupervised learning, where we have a set of unlabeled data, and we are trying to find patterns in the data. Like clustering, classification entails finding patterns in the data and grouping similar data points together. However, we will be using predefined labeled groups rather than trying to find the groups ourselves.\n\nLet's load a dataset to conduct classification work on.\n\n```{python}\n# Load the penguins dataset from the seaborn library\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n```\n\n```{python}\nprint(f\"Number of observations: {penguins.shape[0]}\")\n```\n\n```{python}\npenguins.isna().sum()\n```\n\nAs shown above, there are some none-numerical values in the dataset, and there are some missing values. We will need to deal with these before we can carry out classification work on the dataset. Let's drop the rows that have missing values.\n\n```{python}\npenguins = penguins.dropna()\npenguins.isna().sum()\n```\n```{python}\nprint(f\"Number of observations: {penguins.shape[0]}\")\n```\n\nAs you can see, we have cut the number of observations from 344 to 333, and removed any rows with missing values. Now we will continue on with our classification work.\n\n## Types of classification\nThere are several different classification algorithms that we can use to classify our data. Some of the most common ones include linear classifiers, tree-based classifiers, and neural network classifiers. \nIn this blog post we will be trying two specific algorithms: support vector machines and random forests. We will be using the scikit-learn library to carry out our classification work.\n\nRandom forests are a type of tree-based classifier, which use multiple decision trees to classify data, which helps improve accuracy and reduce overfitting.\n\nSupport vector machines (SVM) are a type of linear classifier, which find a hyperplane that best separates data into different classes while maximizing the margin between the hyperplane and the data points. SVMs are a very popular classification algorithm, and they are used in both linear and non-linear classification problems. \nWe will start with support vector machines.\n\n## SVM classification\n```{python}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Replace categorical target variable with numerical values\nlabel_encoder = LabelEncoder()\npenguins[\"species_encoded\"] = label_encoder.fit_transform(penguins[\"species\"])\n\n# Obtain separate feature and target sets (X and y)\ny = penguins[\"species_encoded\"]\nX = penguins.drop([\"species\", \"species_encoded\"], axis=1)\n\n```\n\nAs you can see, we have done some basic preprocessing on the dataset, including converting the target variable to numberical values.\nNext we need to deal with the categorical variables in the dataset. We will use label encoding to convert the `sex` feature into numerical values, because this can simply be done for binary or naturally ordinal variables. \nHowever, the island variable is not binary nor does it follow a naturally ordinal state, so to keep it in without creating false relationships in the data we would likely want to use one-hot encoding.\nOne-hot encoding essentially entails creating a new binary column for each possible value of the categorical variable, and assigning a 1 or 0 to each observation depending on which value it has for that variable. \nHowever in this case we will simply drop the island variable from the dataset.\n\n```{python}\npenguins[\"sex_encoded\"] = label_encoder.fit_transform(penguins[\"sex\"])\nX = penguins.drop([\"species\", \"species_encoded\", \"island\", \"sex\"], axis=1)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX.head()\n```\n\nWe have now finished our data preprocessing work, and splitting up our training and testing sets. \nNow we can create our SVM classifier and fit it to the training data.\n\n```{python}\n# Create an SVM classifier\nsvm_classifier = SVC(kernel='linear')\n\n# Fit the classifier on the training data\nsvm_classifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = svm_classifier.predict(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(classification_report(y_test, y_pred))\n```\n\nAs we can see from the results above, our SVM classifier has an accuracy of 0.99, which is quite good.\nLet's help visualize the results by creating a confusion matrix.\n\n```{python}\nimport sklearn.metrics\nimport matplotlib.pyplot as plt\n\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=penguins[\"species\"].unique())\nconf_mat_display.plot()\nplt.show()\n```\n\nAs we can see from the confusion matrix, our SVM classifier did a very good job of classifying the data, with only a single false positive. The model predicted a Chinstrap penguin as an Adelie penguin, but that was the only failure in the dataset. \n\nAlthough this worked quite well already, let's try another classification algorithm to see how the results compare. \n\n## Random forest classification\nAs mentioned before, we are going to apply a random forest classification to the data to see how it compares to the SVM classifier.\nRandom forest classification works by creating multiple decision trees, and then using the mode of the predictions of the individual trees as the final prediction.\nWe already did preprocessing on the data and split it into training and testing sets, so we can simply create a new classifier and fit it to the training data.\n```{python}\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create a Random Forest classifier\nrandom_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Fit the classifier on the training data & make predictions\nrandom_forest_classifier.fit(X_train, y_train)\ny_pred = random_forest_classifier.predict(X_test)\n\n# Evaluate performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(classification_report(y_test, y_pred))\n```\n\n```{python}\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=penguins[\"species\"].unique())\nconf_mat_display.plot()\nplt.show()\n```\n\nThis classifier also did a great job, even beating out the SVM in terms of accuracy. For this relatively simple dataset, random forest classifcation was able to achieve 100% accuracy on our testing data.\n\nLastly, lets create another visualization of the classification, this time visualizing the first three trees of our random forest classifier.\n\n```{python}\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nimport graphviz\n\nfor i in range(3):\n    tree = random_forest_classifier.estimators_[i]\n    dot_data = export_graphviz(tree,\n                               feature_names=X_train.columns,  \n                               filled=True,  \n                               max_depth=2, \n                               impurity=False, \n                               proportion=True)\n    graph = graphviz.Source(dot_data)\n    display(graph)\n```\n\nThese decision trees show how for a given feature, the tree splits the data into two groups based on the value of that feature, and narrows down the possible classes for each group. \nBy combining the results of multiple trees, random forest classification is able to achieve a more comprehensive classification than a single decision tree.\n\n# Conclusion\nIn this blog post, we delved into the concept of classification in machine learning.\nWe looked at the difference between classification and the concepts of regression and clustering, and we looked at some different types of classification. \nWe then worked on a real use case of classification algorithms. \nI hope that this blog post has been helpful in understanding classificaiton in machine learning, and encourages you to continue with your own work in this area.\n\n\n## Sources used\n1. https://github.com/ageron/handson-ml3/tree/main\n2. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n3. https://www.datacamp.com/tutorial/svm-classification-scikit-learn-python\n4. https://www.datacamp.com/tutorial/random-forests-classifier-python\n5. https://www.geeksforgeeks.org/random-forest-classifier-using-scikit-learn/"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"flatly","title-block-banner":true,"title":"Classification","author":"Jonathan West","date":"2023-11-24","categories":["Classification","code"],"description":"This is a blog post demonstrating the concept of classification in machine learning."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}