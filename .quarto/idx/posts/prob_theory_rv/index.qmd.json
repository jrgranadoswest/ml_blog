{"title":"Probability Theory & Random Variables","markdown":{"yaml":{"title":"Probability Theory & Random Variables","author":"Jonathan West","date":"2023-11-21","categories":["Probability theory","Random variables","code"],"description":"This blog post demonstrates concepts from probability theory and random variables.","format":{"html":{"code-tools":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nProbability theory is a fundamental component of machine learning, as it covers the study of uncertainty and randomness. It entails the work of making predictions in the absence of complete information. In this blog post we will walk through some concepts of probability theory, show how probability theory and random variables will be important moving forward in our study of machine learning. In particular, we will discuss logistic regression and naive bayes classifiers.\n\n\n### Probability Distribution example\nOne key aspect of probability theory is the probability distribution, which is essentially a mathematical function that provides the probabilities of occurrence of different possible outcomes in a given scenario. For instance, if we were to flip a coin, the probability distribution would be 50% heads and 50% tails. To get started, we will generate two sets of random data, to demonstrate the uniform and normal probability distributions.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate random data from two distributions\ndata_unif = np.random.uniform(0, 1, 800)  # Uniform\ndata_norm = np.random.normal(0, 1, 800)  # Normal\n\nplt.figure(figsize=(6, 8))  # Create figure with two subplots\n\n# Uniform Distribution\nplt.subplot(2, 1, 1)\nplt.hist(data_unif, bins=20, density=True, alpha=0.6, color='green', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Uniform Probability Distribution')\n\n# Normal Distribution\nplt.subplot(2, 1, 2)\nplt.hist(data_norm, bins=20, density=True, alpha=0.6, color='purple', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Normal Probability Distribution')\n\nplt.tight_layout()  # Avoid overlapping graphs\nplt.show()  # Display\n```\n\nAs you can see above, the uniform distribution appears as a rough rectangle, while the normal distribution appears as a bell curve. These are two common probability distributions, where outcomes are all equally likely, and where outcomes are more likely to be near the mean, respectively. Next lets apply some probability theory to a more practical set of data.\n\n### Load and examine data \nFirst we will load the `iris` dataset from scikit-learn, and examine its features.\n```{python}\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris_data = load_iris()\n# Display some basic information about the dataset\nprint(\"Iris Dataset:\")\nprint(f\"{len(iris_data.target_names)} Target names: {', '.join(iris_data.target_names)}\")\nprint(f\"{len(iris_data.feature_names)} Feature names: {', '.join(iris_data.feature_names)}\")\nprint(f\"Data shape: {iris_data.data.shape}\")\n```\n\n```{python}\nimport pandas as pd\n# Display first 5 rows of data\ndf = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\ndf[\"target\"] = iris_data.target\n# Iterate over data frame and replace target values with target names\nfor target, target_name in enumerate(iris_data.target_names):\n    df[\"target\"] = df[\"target\"].replace(target, target_name)\ndf.head()\n```\n\nAs shown above, this dataset contains 150 observations, with 4 features, and 3 target classes. By loading the dataset into a pandas dataframe, we can easily visualize the data and its features. \n\n\nFirst, lets visualize the data with a kernel density estimate (KDE) plot, which allows us to visualize the distribution of observations in a dataset. The KDE plot is a smoothed version of the histogram, which shows the probability density function (PDF) of the data.\n\n```{python}\nimport seaborn as sns\n# KDE plot of data \nsns.displot(df, x=\"sepal length (cm)\", kde=True, hue=\"target\", stat=\"density\")\n```\n\nAbove we can see the KDE plot of the sepal length feature, with the target classes colored. The \"stat\" argument, in this case set to \"density\", means that the KDE plot will be normalized such that the area under the curve is equal to 1.\nThis graph allows us to understand the probability distribution of our data in terms of the target classes. We can see that the setosa class has a much smaller sepal length than the other two classes.\n\n```{python}\nsns.displot(df, x=\"sepal width (cm)\", kde=True, hue=\"target\", stat=\"count\")\n```\n\nWith this second graph, we can see that the sepal width feature is not as useful for distinguishing between the target classes, as the distributions are very similar. However, the setosa class does appear to have a slightly larger sepal width than the other two, and the versicolor class appears to have a slightly smaller sepal width than the other two. By accounting for all of the features, we can create a more accurate model for predicting the target classes, even when certain features are not as useful on their own. We will discuss this type of work in future blog posts, but we will show a simple way to predict the target classes from the data using logistic regression, and a naïve bayes classifier.\n\n### Logistic Regression\nLogistic regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes, like true or false. However, it can also handle multi-class classification problems by using techniques like one-vs-all (OvA) or softmax regression. \"One-vs-all\" is exactly what it sounds like; we train a binary classifier for each class, to just predict if a given data point belongs to that class or any of the others.\n\nLogistic regression is based on the idea of modeling the probability of an observation belonging to a particular class. It essentially predicts the probability of an observation belonging to a specific category or class. We use the sigmoid function to map the linear combination of input features to a probability score between 0 and 1, because the sigmoid function is bounded by 0 and 1, and conveniently approaches 1 as the input approaches infinity, and approaches 0 as the input approaches negative infinity. \n\nLogistic regression works by calculating a linear combination of the input features, then applying the sigmoid function to this value to get the probability of the observation belonging to a particular class. We can define a numerical boundary to classify probability scores as indicating one class or the other. \n\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nX = iris_data.data  # Feature variables\ny = iris_data.target  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create model and fit with data\nlog_reg_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg_model.fit(X_train, y_train)\n\n# Predict target values of test data using the trained model\ny_pred = log_reg_model.predict(X_test)\n\n# Examine performance of model\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\n```\n\nThe confusion matrix is a common way to display classification results. It shows the number of true positives, false positives, true negatives, and false negatives. In this case we obtained a 3 by 3 grid due to the 3 target classes. The diagonal of the matrix shows the number of true negatives, while the off-diagonal elements show the number of incorrect predictions, specifically what class was correct and what class it was predicted as. \n\nIn this case with a fairly straightforward dataset, you can see that our model had false positives on our test data.\n\nLets visualize the confusion matrix with a heatmap.\n\n```{python}\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n```\n\n\n### Naïve Bayes Classifier\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naïve) independence assumptions. \nIt will allow us to classify the data into predicted target classes, based on the probability of the data belonging to each class, judged by the features. \nNaïve Bayes classifiers have found popularity for text classification, for example in dealing with spam detection. \n\nIt considers the probability of an observation belonging to a particular class, given the features of the observation, and classifies the observation as the class with the highest probability. Lets use the same iris dataset as before, and see how well the naïve bayes classifier does.\n\n```{python}\nfrom sklearn.naive_bayes import GaussianNB\n# Initialize the Naive Bayes classifier (GaussianNB is used for continuous data), and fit to training data\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = classifier.predict(X_test)\n\n# Examine performance of classifier\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nclass_report = sklearn.metrics.classification_report(y_test, y_pred, target_names=iris_data.target_names)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\nprint(\"Classification Report:\")\nprint(class_report)\n```\n\nAs you can see, the naïve bayes classifier performed fairly well on this dataset, although not as well as the logistic regression model.\nFinally, lets visualize the confusion matrix with a heatmap.\n\n```{python}\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n```\n\n# Conclusion\nIn this blog post, we jumped into the concepts of probability theory and random variables which are essential to machine learning.\nProbability theory enables us to model uncertainty, make informed decisions, and construct intelligent systems. \nRandom variables allow us to quantify and analyze various aspects of the world and the patterns and randomness which occur. \nThese concepts are applied in many machine learning algorithms, as exemplified with logistic regression and naïve bayes classifiers.\n\nI hope this blog post has been helpful in understanding these concepts, and I hope you return for future machine learning blog posts.\n\n## References\n1. https://numpy.org/doc/stable/reference/random/generator.html\n2. https://matplotlib.org/stable/gallery/statistics/hist.html\n3. https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n4. https://www.datacamp.com/tutorial/understanding-logistic-regression-python\n5. https://maptv.github.io/blog/prob/\n6. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html","srcMarkdownNoYaml":"\n\n## Introduction\nProbability theory is a fundamental component of machine learning, as it covers the study of uncertainty and randomness. It entails the work of making predictions in the absence of complete information. In this blog post we will walk through some concepts of probability theory, show how probability theory and random variables will be important moving forward in our study of machine learning. In particular, we will discuss logistic regression and naive bayes classifiers.\n\n\n### Probability Distribution example\nOne key aspect of probability theory is the probability distribution, which is essentially a mathematical function that provides the probabilities of occurrence of different possible outcomes in a given scenario. For instance, if we were to flip a coin, the probability distribution would be 50% heads and 50% tails. To get started, we will generate two sets of random data, to demonstrate the uniform and normal probability distributions.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Generate random data from two distributions\ndata_unif = np.random.uniform(0, 1, 800)  # Uniform\ndata_norm = np.random.normal(0, 1, 800)  # Normal\n\nplt.figure(figsize=(6, 8))  # Create figure with two subplots\n\n# Uniform Distribution\nplt.subplot(2, 1, 1)\nplt.hist(data_unif, bins=20, density=True, alpha=0.6, color='green', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Uniform Probability Distribution')\n\n# Normal Distribution\nplt.subplot(2, 1, 2)\nplt.hist(data_norm, bins=20, density=True, alpha=0.6, color='purple', edgecolor='black')\nplt.xlabel('Random Variable Values')\nplt.ylabel('Probability Density')\nplt.title('Normal Probability Distribution')\n\nplt.tight_layout()  # Avoid overlapping graphs\nplt.show()  # Display\n```\n\nAs you can see above, the uniform distribution appears as a rough rectangle, while the normal distribution appears as a bell curve. These are two common probability distributions, where outcomes are all equally likely, and where outcomes are more likely to be near the mean, respectively. Next lets apply some probability theory to a more practical set of data.\n\n### Load and examine data \nFirst we will load the `iris` dataset from scikit-learn, and examine its features.\n```{python}\nfrom sklearn.datasets import load_iris\n\n# Load iris dataset\niris_data = load_iris()\n# Display some basic information about the dataset\nprint(\"Iris Dataset:\")\nprint(f\"{len(iris_data.target_names)} Target names: {', '.join(iris_data.target_names)}\")\nprint(f\"{len(iris_data.feature_names)} Feature names: {', '.join(iris_data.feature_names)}\")\nprint(f\"Data shape: {iris_data.data.shape}\")\n```\n\n```{python}\nimport pandas as pd\n# Display first 5 rows of data\ndf = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\ndf[\"target\"] = iris_data.target\n# Iterate over data frame and replace target values with target names\nfor target, target_name in enumerate(iris_data.target_names):\n    df[\"target\"] = df[\"target\"].replace(target, target_name)\ndf.head()\n```\n\nAs shown above, this dataset contains 150 observations, with 4 features, and 3 target classes. By loading the dataset into a pandas dataframe, we can easily visualize the data and its features. \n\n\nFirst, lets visualize the data with a kernel density estimate (KDE) plot, which allows us to visualize the distribution of observations in a dataset. The KDE plot is a smoothed version of the histogram, which shows the probability density function (PDF) of the data.\n\n```{python}\nimport seaborn as sns\n# KDE plot of data \nsns.displot(df, x=\"sepal length (cm)\", kde=True, hue=\"target\", stat=\"density\")\n```\n\nAbove we can see the KDE plot of the sepal length feature, with the target classes colored. The \"stat\" argument, in this case set to \"density\", means that the KDE plot will be normalized such that the area under the curve is equal to 1.\nThis graph allows us to understand the probability distribution of our data in terms of the target classes. We can see that the setosa class has a much smaller sepal length than the other two classes.\n\n```{python}\nsns.displot(df, x=\"sepal width (cm)\", kde=True, hue=\"target\", stat=\"count\")\n```\n\nWith this second graph, we can see that the sepal width feature is not as useful for distinguishing between the target classes, as the distributions are very similar. However, the setosa class does appear to have a slightly larger sepal width than the other two, and the versicolor class appears to have a slightly smaller sepal width than the other two. By accounting for all of the features, we can create a more accurate model for predicting the target classes, even when certain features are not as useful on their own. We will discuss this type of work in future blog posts, but we will show a simple way to predict the target classes from the data using logistic regression, and a naïve bayes classifier.\n\n### Logistic Regression\nLogistic regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes, like true or false. However, it can also handle multi-class classification problems by using techniques like one-vs-all (OvA) or softmax regression. \"One-vs-all\" is exactly what it sounds like; we train a binary classifier for each class, to just predict if a given data point belongs to that class or any of the others.\n\nLogistic regression is based on the idea of modeling the probability of an observation belonging to a particular class. It essentially predicts the probability of an observation belonging to a specific category or class. We use the sigmoid function to map the linear combination of input features to a probability score between 0 and 1, because the sigmoid function is bounded by 0 and 1, and conveniently approaches 1 as the input approaches infinity, and approaches 0 as the input approaches negative infinity. \n\nLogistic regression works by calculating a linear combination of the input features, then applying the sigmoid function to this value to get the probability of the observation belonging to a particular class. We can define a numerical boundary to classify probability scores as indicating one class or the other. \n\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nimport sklearn\n\nX = iris_data.data  # Feature variables\ny = iris_data.target  # Target variable\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\n# create model and fit with data\nlog_reg_model = LogisticRegression(random_state=42, max_iter=1000)\nlog_reg_model.fit(X_train, y_train)\n\n# Predict target values of test data using the trained model\ny_pred = log_reg_model.predict(X_test)\n\n# Examine performance of model\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\n```\n\nThe confusion matrix is a common way to display classification results. It shows the number of true positives, false positives, true negatives, and false negatives. In this case we obtained a 3 by 3 grid due to the 3 target classes. The diagonal of the matrix shows the number of true negatives, while the off-diagonal elements show the number of incorrect predictions, specifically what class was correct and what class it was predicted as. \n\nIn this case with a fairly straightforward dataset, you can see that our model had false positives on our test data.\n\nLets visualize the confusion matrix with a heatmap.\n\n```{python}\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n```\n\n\n### Naïve Bayes Classifier\nThe Naïve Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong (naïve) independence assumptions. \nIt will allow us to classify the data into predicted target classes, based on the probability of the data belonging to each class, judged by the features. \nNaïve Bayes classifiers have found popularity for text classification, for example in dealing with spam detection. \n\nIt considers the probability of an observation belonging to a particular class, given the features of the observation, and classifies the observation as the class with the highest probability. Lets use the same iris dataset as before, and see how well the naïve bayes classifier does.\n\n```{python}\nfrom sklearn.naive_bayes import GaussianNB\n# Initialize the Naive Bayes classifier (GaussianNB is used for continuous data), and fit to training data\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = classifier.predict(X_test)\n\n# Examine performance of classifier\nconfusion_matrix = sklearn.metrics.confusion_matrix(y_test, y_pred)\nclass_report = sklearn.metrics.classification_report(y_test, y_pred, target_names=iris_data.target_names)\nprint(\"Confusion Matrix:\\n\", confusion_matrix)\nprint(\"Classification Report:\")\nprint(class_report)\n```\n\nAs you can see, the naïve bayes classifier performed fairly well on this dataset, although not as well as the logistic regression model.\nFinally, lets visualize the confusion matrix with a heatmap.\n\n```{python}\nconf_mat_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=iris_data.target_names)\nconf_mat_display.plot()\nplt.show()\n```\n\n# Conclusion\nIn this blog post, we jumped into the concepts of probability theory and random variables which are essential to machine learning.\nProbability theory enables us to model uncertainty, make informed decisions, and construct intelligent systems. \nRandom variables allow us to quantify and analyze various aspects of the world and the patterns and randomness which occur. \nThese concepts are applied in many machine learning algorithms, as exemplified with logistic regression and naïve bayes classifiers.\n\nI hope this blog post has been helpful in understanding these concepts, and I hope you return for future machine learning blog posts.\n\n## References\n1. https://numpy.org/doc/stable/reference/random/generator.html\n2. https://matplotlib.org/stable/gallery/statistics/hist.html\n3. https://seaborn.pydata.org/generated/seaborn.kdeplot.html\n4. https://www.datacamp.com/tutorial/understanding-logistic-regression-python\n5. https://maptv.github.io/blog/prob/\n6. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"flatly","title-block-banner":true,"title":"Probability Theory & Random Variables","author":"Jonathan West","date":"2023-11-21","categories":["Probability theory","Random variables","code"],"description":"This blog post demonstrates concepts from probability theory and random variables."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}