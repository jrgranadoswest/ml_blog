{"title":"Anomaly/Outlier Detection","markdown":{"yaml":{"title":"Anomaly/Outlier Detection","author":"Jonathan West","date":"2023-11-25","categories":["Outlier detection","code"],"description":"This blog demonstrates anomaly and outlier detection.","format":{"html":{"code-tools":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nAnomaly detection is the process of identifying data points that are notably different from the rest of the data. \nObservations can be skewed due to erroneous data collection, a change in system behavior, or simply an unusual yet natural event. \nAnomalies/outliers can be applied to applications such as fraud detection or discovering defects in manufactured products. \nAdditionally, it is worth taking note of outlier detection because it can be important to remove outliers from data as a preprocessing step, for a subsequent machine learning algorithm to perform well.\n\nFirst, we are going to work with a \"toy\" dataset, by generating data from the scikit learn make_blobs function.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a dataset with clusters, and add some arbitrary values as outliers\nX, y = make_blobs(n_samples=400, centers=3, cluster_std=1.3, random_state=42)\noutliers = np.array([[7, 12], [1, -10], [9, 11]])\nX = np.vstack((X, outliers))\nX.shape\n```\n\nThe make_blobs function lets us generate Gaussiang blobs for clustering. We generated 400 samples, with three centers and a standard deviation of 1.3. \nWe also added three arbitrarily chosen outlier points to the dataset, to practice anomaly detection.\nNext, let's fit a gaussian mixture model to the data, and plot the results.\n\n## Gaussian Mixture Model \n```{python}\n# Fit a Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X)\n\n# Predict the labels and probabilities\nlabels = gmm.predict(X)\nprobs = gmm.predict_proba(X)\n\n# Plot the data points, color-coded by cluster\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.colorbar()\n\nplt.title(\"Gaussian Mixture Model with Outliers\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n```\n\nIt is fairly easy to visually identify the three outlier points that we added to the dataset. \nHowever, in a multi-dimensional dataset, it is not as easy to identify outliers, and in either case mathematical methods are needed to identify outliers and justify the classification.\nLet's identify the outliers in our generated data. \n\n## Identifying Outliers with Gaussian Mixture Models\n\nGaussian mixture models essentially work by fitting a number of gaussian distributions to the data, and then using the probabilities of each point belonging to each distribution to classify the points. \nIt is important to note that each observation in the data is therefore assigned a probability of belonging to each distribution. \nBy finding which observations have the lowest probability of belonging to any distribution, we can identify the outliers in the data.\n\n```{python}\n# Set a threshold for outlier identification\nthreshold = 0.999\n\n# Find outliers based on the maximum probability of any distribution being below our cutoff\noutlier_indices = np.where(np.max(probs, axis=1) < threshold)\n\n# Plot the outliers\nplt.scatter(X[outlier_indices, 0], X[outlier_indices, 1], marker='o', color='r', s=100, label='Outliers')\n# Replot the rest of the data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.colorbar()\n```\n\nWe can see that two of the three outlier points we added are identified as outliers, and the third is not. \nAdditionally, the threshold was set quite high to consider the two points as outliers, and in many cases we would likely not consider them as anomalies in real data.\nLet's take a look at the lowest probability points in the data.\n\n```{python}\n# Find 10 data points with lowest probabilities\nlowest_prob_indices = np.argsort(np.max(probs, axis=1))[:10]\nfor i, idx in enumerate(lowest_prob_indices):\n    print(f\"{i+1}th lowest probability, idx {idx}; prob {np.max(probs[idx]):.4f}; point {X[idx]}\")\n```\n\nWe can see that the two outlier points we added are the two points with the lowest probabilities of belonging to any distribution, yet the third attempted outlier was not in the lowest 10 probabilities.\nThis may be because the third data point is closer to the overall center of the data, and therefore has a higher probability of belonging to one of the distributions, despite not being particularly close to any of the cluster centers. \nAdditionally, this highlights the importance of choosing an appropriate threshold and outlier for anomaly detection.\n\n## One Class SVM\nA single class support vector machine (SVM) is a supervised learning algorithm that is used to identify outliers in data.\nAs the name implies, it is a type of SVM that is designed to learn a particular boundary around a set of data, and then identify points that are outside of that boundary.\nIn other words, it finds a hyperplane in a high dimensional space that separates the majority of the data from potential outliers.\n\n\nLet's work with a more realistic dataset to practice this technique.\nWe will be examining the California housing dataset from scikit learn to explore the concept of anomaly and outlier detection. \n\n### Load and summarize the data\n```{python}\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California housing dataset\nhousing_data = fetch_california_housing()\nhd = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nhd['target'] = housing_data.target\nhd.head()\n```\n\n\nAs we have seen in a previous blog, this dataset contains housing data from California with the target variable being the median house value in hundreds of thousands of dollars.\nThere are eight features in this dataset. \n\nWe can visualize the data by using PCA for dimensionality reduction, and then plotting the first two principal components.\n\n```{python}\nfrom sklearn.decomposition import PCA\n\n# Fit PCA to the data\npca = PCA(n_components=2)\npca.fit(housing_data.data)\n\n# Transform the data\nX_pca = pca.transform(housing_data.data)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=housing_data.target[:], cmap='plasma', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\ncol_bar = plt.colorbar()\ncol_bar.set_label('House Value')\nplt.show()\n```\n\n\nWe can see that there are some data points that definitely stand out from the rest of the data, based on principal component analysis. \nLet's get an evaluation of how well a linear regression model performs on this data, and then we will remove the outliers and see how the model performs.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n# split into input and output elements\nX = housing_data.data\ny = housing_data.target\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n```\n\nNow we will use a one class SVM to identify the outliers.\n\n```{python}\n# evaluate model performance with outliers removed using one class SVM\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import mean_absolute_error\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n# identify outliers in the training dataset\nee = OneClassSVM(nu=0.01)\nyhat = ee.fit_predict(X_train)\n\n# select all rows that are not outliers\nmask = yhat != -1\nprint(\"Pre-outlier removal data shape:\", X_train.shape, y_train.shape)  # Shape of data before removing outliers\nX_train, y_train = X_train[mask, :], y_train[mask]\nprint(\"Post-outlier removal data shape:\", X_train.shape, y_train.shape)  # Shape of data after removing outliers\n\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n```\n\nIt appears that a significant number of outliers were removed from the data.\nWe can see that the mean absolute error of the model is reduced by removing the outliers from the data, however not by much in this case. \nFinally, we will make a plot of the data with the outliers removed.\n\n```{python}\nfrom sklearn.decomposition import PCA\n\n# Fit PCA to the data\npca = PCA(n_components=2)\npca.fit(housing_data.data)\n\n# Transform the data\nX_pca = pca.transform(X_train)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=y_train[:], cmap='plasma', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\ncol_bar = plt.colorbar()\ncol_bar.set_label('House Value')\nplt.show()\n```\n\nIt is evident that with outliers removed, the data is much more tightly clustered, and this visualization allows us to view the data more clearly.\n\n## Conclusion\nIn this blog, we jumped into the concept of anomaly and outlier detection. \nThere are a variety of different techniques that can be used to identify outliers in data, and the choice of technique depends on the type of data and the application. \nIn this case, we practiced the use of Gaussian mixture models and one class SVMs to identify outliers in data.\nIf you are looking to further your knowledge on this subject, consider applying these techniques to a dataset of your choice, or exploring other techniques such as isolation forests or local outlier factor.\n\n## Sources used\n1. https://github.com/ageron/handson-ml3\n2. https://scikit-learn.org/stable/modules/outlier_detection.html\n3. https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/\n4. https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/","srcMarkdownNoYaml":"\n\n## Introduction\nAnomaly detection is the process of identifying data points that are notably different from the rest of the data. \nObservations can be skewed due to erroneous data collection, a change in system behavior, or simply an unusual yet natural event. \nAnomalies/outliers can be applied to applications such as fraud detection or discovering defects in manufactured products. \nAdditionally, it is worth taking note of outlier detection because it can be important to remove outliers from data as a preprocessing step, for a subsequent machine learning algorithm to perform well.\n\nFirst, we are going to work with a \"toy\" dataset, by generating data from the scikit learn make_blobs function.\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a dataset with clusters, and add some arbitrary values as outliers\nX, y = make_blobs(n_samples=400, centers=3, cluster_std=1.3, random_state=42)\noutliers = np.array([[7, 12], [1, -10], [9, 11]])\nX = np.vstack((X, outliers))\nX.shape\n```\n\nThe make_blobs function lets us generate Gaussiang blobs for clustering. We generated 400 samples, with three centers and a standard deviation of 1.3. \nWe also added three arbitrarily chosen outlier points to the dataset, to practice anomaly detection.\nNext, let's fit a gaussian mixture model to the data, and plot the results.\n\n## Gaussian Mixture Model \n```{python}\n# Fit a Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X)\n\n# Predict the labels and probabilities\nlabels = gmm.predict(X)\nprobs = gmm.predict_proba(X)\n\n# Plot the data points, color-coded by cluster\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.colorbar()\n\nplt.title(\"Gaussian Mixture Model with Outliers\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n```\n\nIt is fairly easy to visually identify the three outlier points that we added to the dataset. \nHowever, in a multi-dimensional dataset, it is not as easy to identify outliers, and in either case mathematical methods are needed to identify outliers and justify the classification.\nLet's identify the outliers in our generated data. \n\n## Identifying Outliers with Gaussian Mixture Models\n\nGaussian mixture models essentially work by fitting a number of gaussian distributions to the data, and then using the probabilities of each point belonging to each distribution to classify the points. \nIt is important to note that each observation in the data is therefore assigned a probability of belonging to each distribution. \nBy finding which observations have the lowest probability of belonging to any distribution, we can identify the outliers in the data.\n\n```{python}\n# Set a threshold for outlier identification\nthreshold = 0.999\n\n# Find outliers based on the maximum probability of any distribution being below our cutoff\noutlier_indices = np.where(np.max(probs, axis=1) < threshold)\n\n# Plot the outliers\nplt.scatter(X[outlier_indices, 0], X[outlier_indices, 1], marker='o', color='r', s=100, label='Outliers')\n# Replot the rest of the data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.colorbar()\n```\n\nWe can see that two of the three outlier points we added are identified as outliers, and the third is not. \nAdditionally, the threshold was set quite high to consider the two points as outliers, and in many cases we would likely not consider them as anomalies in real data.\nLet's take a look at the lowest probability points in the data.\n\n```{python}\n# Find 10 data points with lowest probabilities\nlowest_prob_indices = np.argsort(np.max(probs, axis=1))[:10]\nfor i, idx in enumerate(lowest_prob_indices):\n    print(f\"{i+1}th lowest probability, idx {idx}; prob {np.max(probs[idx]):.4f}; point {X[idx]}\")\n```\n\nWe can see that the two outlier points we added are the two points with the lowest probabilities of belonging to any distribution, yet the third attempted outlier was not in the lowest 10 probabilities.\nThis may be because the third data point is closer to the overall center of the data, and therefore has a higher probability of belonging to one of the distributions, despite not being particularly close to any of the cluster centers. \nAdditionally, this highlights the importance of choosing an appropriate threshold and outlier for anomaly detection.\n\n## One Class SVM\nA single class support vector machine (SVM) is a supervised learning algorithm that is used to identify outliers in data.\nAs the name implies, it is a type of SVM that is designed to learn a particular boundary around a set of data, and then identify points that are outside of that boundary.\nIn other words, it finds a hyperplane in a high dimensional space that separates the majority of the data from potential outliers.\n\n\nLet's work with a more realistic dataset to practice this technique.\nWe will be examining the California housing dataset from scikit learn to explore the concept of anomaly and outlier detection. \n\n### Load and summarize the data\n```{python}\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California housing dataset\nhousing_data = fetch_california_housing()\nhd = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nhd['target'] = housing_data.target\nhd.head()\n```\n\n\nAs we have seen in a previous blog, this dataset contains housing data from California with the target variable being the median house value in hundreds of thousands of dollars.\nThere are eight features in this dataset. \n\nWe can visualize the data by using PCA for dimensionality reduction, and then plotting the first two principal components.\n\n```{python}\nfrom sklearn.decomposition import PCA\n\n# Fit PCA to the data\npca = PCA(n_components=2)\npca.fit(housing_data.data)\n\n# Transform the data\nX_pca = pca.transform(housing_data.data)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=housing_data.target[:], cmap='plasma', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\ncol_bar = plt.colorbar()\ncol_bar.set_label('House Value')\nplt.show()\n```\n\n\nWe can see that there are some data points that definitely stand out from the rest of the data, based on principal component analysis. \nLet's get an evaluation of how well a linear regression model performs on this data, and then we will remove the outliers and see how the model performs.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n# split into input and output elements\nX = housing_data.data\ny = housing_data.target\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n```\n\nNow we will use a one class SVM to identify the outliers.\n\n```{python}\n# evaluate model performance with outliers removed using one class SVM\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import mean_absolute_error\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n# identify outliers in the training dataset\nee = OneClassSVM(nu=0.01)\nyhat = ee.fit_predict(X_train)\n\n# select all rows that are not outliers\nmask = yhat != -1\nprint(\"Pre-outlier removal data shape:\", X_train.shape, y_train.shape)  # Shape of data before removing outliers\nX_train, y_train = X_train[mask, :], y_train[mask]\nprint(\"Post-outlier removal data shape:\", X_train.shape, y_train.shape)  # Shape of data after removing outliers\n\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n```\n\nIt appears that a significant number of outliers were removed from the data.\nWe can see that the mean absolute error of the model is reduced by removing the outliers from the data, however not by much in this case. \nFinally, we will make a plot of the data with the outliers removed.\n\n```{python}\nfrom sklearn.decomposition import PCA\n\n# Fit PCA to the data\npca = PCA(n_components=2)\npca.fit(housing_data.data)\n\n# Transform the data\nX_pca = pca.transform(X_train)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=y_train[:], cmap='plasma', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\ncol_bar = plt.colorbar()\ncol_bar.set_label('House Value')\nplt.show()\n```\n\nIt is evident that with outliers removed, the data is much more tightly clustered, and this visualization allows us to view the data more clearly.\n\n## Conclusion\nIn this blog, we jumped into the concept of anomaly and outlier detection. \nThere are a variety of different techniques that can be used to identify outliers in data, and the choice of technique depends on the type of data and the application. \nIn this case, we practiced the use of Gaussian mixture models and one class SVMs to identify outliers in data.\nIf you are looking to further your knowledge on this subject, consider applying these techniques to a dataset of your choice, or exploring other techniques such as isolation forests or local outlier factor.\n\n## Sources used\n1. https://github.com/ageron/handson-ml3\n2. https://scikit-learn.org/stable/modules/outlier_detection.html\n3. https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/\n4. https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"flatly","title-block-banner":true,"title":"Anomaly/Outlier Detection","author":"Jonathan West","date":"2023-11-25","categories":["Outlier detection","code"],"description":"This blog demonstrates anomaly and outlier detection."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}