{"title":"Linear & Nonlinear regression","markdown":{"yaml":{"title":"Linear & Nonlinear regression","author":"Jonathan West","date":"2023-11-23","categories":["Regression","code"],"description":"This is a blog post demonstrating linear and nonlinear regression.","format":{"html":{"code-tools":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nLinear and nonlinear regression are important foundational techniques in machine learning and statistics used to model the relationship between independent and dependent variables in data. \nLinear regression is an approach to model the relationship between a development variable and one or more explanatory variables by fitting a linear equation to observed data. \nNonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. \nNonlinear regression allows for more complex relationships between variables, and is essential for identifying and modeling patterns in the data that are nonlinear. \n\nIn today's blog post, we will be exploring linear and nonlinear regression using Python, with the scikit-learn California housing dataset.\n\n## Linear regression\nAs mentioned above, linear regression is a linear approach to modeling the relationship between a development variable and one or more explanatory variables.\nWe can use linear regression to predict the value of future dependent variable observations, by modeling on recorded data.\n\nLet's start by conducting simple linear regression on some randomly generated data. We will use the `numpy` library to create a dataset randomly distributed around the line `y = 2.5x + 1.\n\n```{python}\n#| code-fold: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample data\nnp.random.seed(42)\nX = 2.5 * np.random.randn(100) + 2  # Array of 100 values centered at 2 stddev = 3\ny = 0.5 * X + np.random.randn(100)  # Linear equation y = 0.5x, with added random noise\nplt.scatter(X, y, color='blue')\nplt.title('Simple Linear Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n```\n\nHere we have generated a dataset of 100 points, with a linear relationship y = 0.5x - 3, and random noise added to the data.\nNow we will conduct simple linear regression to fit a line to the data, and make predictions.\n\n```{python}\n# Create a linear regression model and fit it to the data\n# Find means of x & y\nx_mean = np.mean(X)\ny_mean = np.mean(y)\n\n# Calculate terms needed for the numerator and denominator of predicted slope \nnumerator = np.sum((X - x_mean) * (y - y_mean))\ndenominator = np.sum((X - x_mean) ** 2)\n\n# Calculate the linear equation coefficients\nslope = numerator / denominator\nintercept = y_mean - (slope * x_mean)\n\n# Create predicted line\ny_pred = slope * X + intercept\n\n# Plot the data points and the predicted line\nplt.scatter(X, y, color='green')\nlabel = f'y = {slope:.2f}x + {intercept:.2f}'\nplt.plot(X, y_pred, color='blue')\nplt.title('Simple Linear Regression: ' + label)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n```\n\nWe can see that the line of best fit is very close to the original line, and the predicted line is a good fit for the data. \nThis is a simple toy example of linear regression, which is based in basic algebra and uses the least squares method to find the line of best fit.\nThe example worked quite nicely because the data was generated with a linear relationship, with some random noise added.\nNow, lets use the concept of regression to predict the median house value in Californian districts, using the California housing dataset.\n\n```{python}\n#| code-fold: true\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhd = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nhd[housing_data.target_names[0]] = housing_data.target  # Include target variable in dataframe: median house value\n\n# Display first 5 rows of data\nhd.head()\n```\n\nAs usual, we will start by exploring the data. We can see that there are 8 features, and 1 target variable: median house value. \nOur goal is to predict the median house value based on the other features.\nLets start by looking at the distribution of the target variable, median house value in hundreds of thousands of dollars.\n\n```{python}\n# Plot the distribution of the target variable\nsns.distplot(hd[housing_data.target_names[0]])\nplt.title('Distribution of median house value')\nplt.xlabel('Median house value')\nplt.ylabel('Frequency')\nplt.show()\n```\n\nThe median house value is distributed roughly normally, with a mean of around 2.1 and a slight right skew.\nNow lets look at the correlation between the target variable and the other features.\nWe will create scatterplots of the target variable against each feature, and calculate the correlation coefficient.\n\n```{python}\n# Assuming df is your DataFrame with the 9 numerical features\n# and \"MedHouseValue\" is the target feature\n# Select the 8 numerical features you want to plot\nfeatures_to_plot = housing_data.feature_names\n\n# Create a new figure with 8 subplots\nfig, axes = plt.subplots(4, 2, figsize=(8, 16))\nfig.suptitle(\"Scatterplots of Numerical Features vs. Median House Value\", size=14)\n\n# Flatten the 2D array of subplots into a 1D array\naxes = axes.flatten()\n\n# Loop through the selected features and create scatterplots\nfor i, feature in enumerate(features_to_plot):\n    ax = axes[i]\n    ax.scatter(hd[feature], hd[\"MedHouseVal\"], alpha=0.5)\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Median House Value (in $100,000s)\")\n\n# Adjust the layout and display\nplt.tight_layout()\nplt.show()\n```\n\nAs we can see from these graphs, most of the features lack a strong linear relationship with the target variable. The strongest linear relationship is between median income and median house value, which makes sense. Higher income areas tend to have higher house prices.\n\nIn any case, lets use linear regression to predict the median house value based on median income, which seems to be the most promising feature.\n\n```{python}\nfrom sklearn.linear_model import LinearRegression\n\n# Simple linear regression\nX = hd['MedInc'].values\ny = hd['MedHouseVal'].values\n# reshape X\nX = X.reshape(-1, 1)\n\n# Create linear regression model & fit to data\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Obtain equation coefficients, & predict y values\nslope = model.coef_\nintercept = model.intercept_\ny_pred = model.predict(X)\n\n# Display our results\nplt.scatter(X, y, color='green')  # actual points\nplt.plot(X, y_pred, color='blue')  # predicted line\nplt.title('House value based on household income: ' + f'y = {slope[0]:.2f}x + {intercept:.2f}')\nplt.xlabel('Median income')\nplt.ylabel('Median house value')\nplt.show()\n```\n\nAs we can see, the line of best fit roughly correlates to the data, but there are many outliers. Let's calculate the mean squared error to see how well our model fits the data.\n\n```{python}\n# Compute mean squared error\nmse = np.mean((y_pred - y) ** 2)\nprint(f'Mean squared error: {mse}')\n```\n\nIn this case the mean squared error is relatively high, which we could have expected simply by looking at the graph. \nPerhaps the data is not linearly related, and we should try a nonlinear regression model instead. We can now move on to nonlinear regression.\n\n## Nonlinear regression\nNonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. It is a fundamental concept in machine learning, and is used to model complex relationships between variables.\n\nLet's apply nonlinear regression to the California housing dataset, and see if we can improve our predictions of median house value.\n\n```{python}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Obtain data again\nX2 = hd['MedInc'].values\ny2 = hd['MedHouseVal'].values\n\n# Reshape X\nX2 = X2.reshape(-1, 1)\n\n# Create Polynomial Features\ndegree = 2  # You can adjust the degree of the polynomial\ndef show_nonlin_regression(X, y, degree):\n    poly_features = PolynomialFeatures(degree=degree)\n    X_poly = poly_features.fit_transform(X)\n\n    # Create and fit Polynomial Regression model\n    poly_model = LinearRegression()\n    poly_model.fit(X_poly, y)\n\n    # Predict y values\n    y_pred = poly_model.predict(X_poly)\n\n    # Sort X and y_pred for better visualization\n    sort_idx = np.argsort(X[:, 0])\n    X_sorted = X[sort_idx]\n    y_pred_sorted = y_pred[sort_idx]\n\n    # Display results\n    plt.scatter(X, y, color='green', label='Actual data')\n    plt.plot(X_sorted, y_pred_sorted, color='blue', label='Polynomial Regression')\n    plt.title(f'House value based on household income (Polynomial Regression - Degree {degree})')\n    plt.xlabel('Median income')\n    plt.ylabel('Median house value')\n    plt.legend()\n    plt.show()\n    return y_pred\n\ny_pred2 = show_nonlin_regression(X2, y2, degree)\n```\n\nThis time it appears the regression model fits the data slightly better, but not by much.\n\n```{python}\n# Compute mean squared error\nmse = np.mean((y_pred2 - y2) ** 2)\nprint(f'Mean squared error: {mse}')\n```\n\nAs you can see, the mean squared error is slightly lower than the linear regression model, but still quite high. Let's try a higher degree polynomial.\n\n```{python}\n# Obtain data again\nX3 = hd['MedInc'].values\ny3 = hd['MedHouseVal'].values\nX3 = X3.reshape(-1, 1)\ny_pred3 = show_nonlin_regression(X3, y3, 3)\n```\n\n```{python}\n# Compute mean squared error\nmse = np.mean((y_pred2 - y2) ** 2)\nprint(f'Mean squared error: {mse}')\n```\n\nYet again, the model doesn't fit the data incredibly well, and the mean squared error is still quite high. Additionally, it appears that the model is begginning to overfit the data, as the line of best fit is beginning to curve too much to fit the data.\nIt doesn't make sense for very high income areas to have somewhat lower house prices, yet the model predicts this.\nWith simple regression it is difficult to model the relationship between median income and median house value, as there are many other factors that affect house prices. The variance in house prices is likely due to other factors, such as location, house size, and age of the house.\nHowever, we have still illustrated how linear and nonlinear regression can be applied to predict the value of future dependent variable observations, by modeling on recorded data.\n\nLastly, let's try to predict the median house value based on the other features, using multiple linear regression.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Prepare feature matrix and target \nX = hd.drop(columns=['MedHouseVal'])  # Use all columns except 'MedHouseVal' as features\ny = hd['MedHouseVal']\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and fit multiple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Interpret the model coefficients\ncoefficients = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': model.coef_})\nintercept = model.intercept_\nprint(\"Intercept:\", intercept)\nprint(\"Coefficients:\")\nprint(coefficients)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"R-squared (R2) Score:\", r2)\n```\n\nAs we can see, the multiple linear regression model fits the data much better than the simple linear regression model, and the mean squared error is much lower. \nIncorporating multiple features allows us to more closely model the real life relationships between these variables.\nNow lets use a final visualization for the work we have done.\n```{python}\n# Plot actual vs predicted vals\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs. Predicted Values')\nplt.show()\n```\n\nThere does appear to be a linear relationship between the actual and predicted values, following the equation y=x. This is a good sign, as it means our model is predicting the values reasonably. \nHowever we can see that the success of the regression was still limited by our choice of model, data features, and potentially outliers in the data.\n\n# Conclusion\nIn this blog post we have explored linear and nonlinear regression using Python, with the scikit-learn California housing dataset. Linear regression is a linear approach to modeling the relationship between a development variable and one or more explanatory variables, while nonlinear regression is a more general regression analysis in which data is fit to a model and then expressed as a mathematical function. \nRegression is a foundational topic to machine learning, and I hope this blog post has helped you understand some of the basics of regression and inspired you to learn more.\n\n## Sources used\n1. https://realpython.com/linear-regression-in-python/\n2. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html\n3. https://github.com/ageron/handson-ml3\n4. https://www.investopedia.com/terms/m/mlr.asp\n5. https://medium.com/machine-learning-with-python/multiple-linear-regression-implementation-in-python-2de9b303fc0c","srcMarkdownNoYaml":"\n\n## Introduction\nLinear and nonlinear regression are important foundational techniques in machine learning and statistics used to model the relationship between independent and dependent variables in data. \nLinear regression is an approach to model the relationship between a development variable and one or more explanatory variables by fitting a linear equation to observed data. \nNonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. \nNonlinear regression allows for more complex relationships between variables, and is essential for identifying and modeling patterns in the data that are nonlinear. \n\nIn today's blog post, we will be exploring linear and nonlinear regression using Python, with the scikit-learn California housing dataset.\n\n## Linear regression\nAs mentioned above, linear regression is a linear approach to modeling the relationship between a development variable and one or more explanatory variables.\nWe can use linear regression to predict the value of future dependent variable observations, by modeling on recorded data.\n\nLet's start by conducting simple linear regression on some randomly generated data. We will use the `numpy` library to create a dataset randomly distributed around the line `y = 2.5x + 1.\n\n```{python}\n#| code-fold: true\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n# Generate sample data\nnp.random.seed(42)\nX = 2.5 * np.random.randn(100) + 2  # Array of 100 values centered at 2 stddev = 3\ny = 0.5 * X + np.random.randn(100)  # Linear equation y = 0.5x, with added random noise\nplt.scatter(X, y, color='blue')\nplt.title('Simple Linear Regression')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n```\n\nHere we have generated a dataset of 100 points, with a linear relationship y = 0.5x - 3, and random noise added to the data.\nNow we will conduct simple linear regression to fit a line to the data, and make predictions.\n\n```{python}\n# Create a linear regression model and fit it to the data\n# Find means of x & y\nx_mean = np.mean(X)\ny_mean = np.mean(y)\n\n# Calculate terms needed for the numerator and denominator of predicted slope \nnumerator = np.sum((X - x_mean) * (y - y_mean))\ndenominator = np.sum((X - x_mean) ** 2)\n\n# Calculate the linear equation coefficients\nslope = numerator / denominator\nintercept = y_mean - (slope * x_mean)\n\n# Create predicted line\ny_pred = slope * X + intercept\n\n# Plot the data points and the predicted line\nplt.scatter(X, y, color='green')\nlabel = f'y = {slope:.2f}x + {intercept:.2f}'\nplt.plot(X, y_pred, color='blue')\nplt.title('Simple Linear Regression: ' + label)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n```\n\nWe can see that the line of best fit is very close to the original line, and the predicted line is a good fit for the data. \nThis is a simple toy example of linear regression, which is based in basic algebra and uses the least squares method to find the line of best fit.\nThe example worked quite nicely because the data was generated with a linear relationship, with some random noise added.\nNow, lets use the concept of regression to predict the median house value in Californian districts, using the California housing dataset.\n\n```{python}\n#| code-fold: true\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\nhousing_data = fetch_california_housing()\nhd = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nhd[housing_data.target_names[0]] = housing_data.target  # Include target variable in dataframe: median house value\n\n# Display first 5 rows of data\nhd.head()\n```\n\nAs usual, we will start by exploring the data. We can see that there are 8 features, and 1 target variable: median house value. \nOur goal is to predict the median house value based on the other features.\nLets start by looking at the distribution of the target variable, median house value in hundreds of thousands of dollars.\n\n```{python}\n# Plot the distribution of the target variable\nsns.distplot(hd[housing_data.target_names[0]])\nplt.title('Distribution of median house value')\nplt.xlabel('Median house value')\nplt.ylabel('Frequency')\nplt.show()\n```\n\nThe median house value is distributed roughly normally, with a mean of around 2.1 and a slight right skew.\nNow lets look at the correlation between the target variable and the other features.\nWe will create scatterplots of the target variable against each feature, and calculate the correlation coefficient.\n\n```{python}\n# Assuming df is your DataFrame with the 9 numerical features\n# and \"MedHouseValue\" is the target feature\n# Select the 8 numerical features you want to plot\nfeatures_to_plot = housing_data.feature_names\n\n# Create a new figure with 8 subplots\nfig, axes = plt.subplots(4, 2, figsize=(8, 16))\nfig.suptitle(\"Scatterplots of Numerical Features vs. Median House Value\", size=14)\n\n# Flatten the 2D array of subplots into a 1D array\naxes = axes.flatten()\n\n# Loop through the selected features and create scatterplots\nfor i, feature in enumerate(features_to_plot):\n    ax = axes[i]\n    ax.scatter(hd[feature], hd[\"MedHouseVal\"], alpha=0.5)\n    ax.set_xlabel(feature)\n    ax.set_ylabel(\"Median House Value (in $100,000s)\")\n\n# Adjust the layout and display\nplt.tight_layout()\nplt.show()\n```\n\nAs we can see from these graphs, most of the features lack a strong linear relationship with the target variable. The strongest linear relationship is between median income and median house value, which makes sense. Higher income areas tend to have higher house prices.\n\nIn any case, lets use linear regression to predict the median house value based on median income, which seems to be the most promising feature.\n\n```{python}\nfrom sklearn.linear_model import LinearRegression\n\n# Simple linear regression\nX = hd['MedInc'].values\ny = hd['MedHouseVal'].values\n# reshape X\nX = X.reshape(-1, 1)\n\n# Create linear regression model & fit to data\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Obtain equation coefficients, & predict y values\nslope = model.coef_\nintercept = model.intercept_\ny_pred = model.predict(X)\n\n# Display our results\nplt.scatter(X, y, color='green')  # actual points\nplt.plot(X, y_pred, color='blue')  # predicted line\nplt.title('House value based on household income: ' + f'y = {slope[0]:.2f}x + {intercept:.2f}')\nplt.xlabel('Median income')\nplt.ylabel('Median house value')\nplt.show()\n```\n\nAs we can see, the line of best fit roughly correlates to the data, but there are many outliers. Let's calculate the mean squared error to see how well our model fits the data.\n\n```{python}\n# Compute mean squared error\nmse = np.mean((y_pred - y) ** 2)\nprint(f'Mean squared error: {mse}')\n```\n\nIn this case the mean squared error is relatively high, which we could have expected simply by looking at the graph. \nPerhaps the data is not linearly related, and we should try a nonlinear regression model instead. We can now move on to nonlinear regression.\n\n## Nonlinear regression\nNonlinear regression is a form of regression analysis in which data is fit to a model and then expressed as a mathematical function. It is a fundamental concept in machine learning, and is used to model complex relationships between variables.\n\nLet's apply nonlinear regression to the California housing dataset, and see if we can improve our predictions of median house value.\n\n```{python}\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Obtain data again\nX2 = hd['MedInc'].values\ny2 = hd['MedHouseVal'].values\n\n# Reshape X\nX2 = X2.reshape(-1, 1)\n\n# Create Polynomial Features\ndegree = 2  # You can adjust the degree of the polynomial\ndef show_nonlin_regression(X, y, degree):\n    poly_features = PolynomialFeatures(degree=degree)\n    X_poly = poly_features.fit_transform(X)\n\n    # Create and fit Polynomial Regression model\n    poly_model = LinearRegression()\n    poly_model.fit(X_poly, y)\n\n    # Predict y values\n    y_pred = poly_model.predict(X_poly)\n\n    # Sort X and y_pred for better visualization\n    sort_idx = np.argsort(X[:, 0])\n    X_sorted = X[sort_idx]\n    y_pred_sorted = y_pred[sort_idx]\n\n    # Display results\n    plt.scatter(X, y, color='green', label='Actual data')\n    plt.plot(X_sorted, y_pred_sorted, color='blue', label='Polynomial Regression')\n    plt.title(f'House value based on household income (Polynomial Regression - Degree {degree})')\n    plt.xlabel('Median income')\n    plt.ylabel('Median house value')\n    plt.legend()\n    plt.show()\n    return y_pred\n\ny_pred2 = show_nonlin_regression(X2, y2, degree)\n```\n\nThis time it appears the regression model fits the data slightly better, but not by much.\n\n```{python}\n# Compute mean squared error\nmse = np.mean((y_pred2 - y2) ** 2)\nprint(f'Mean squared error: {mse}')\n```\n\nAs you can see, the mean squared error is slightly lower than the linear regression model, but still quite high. Let's try a higher degree polynomial.\n\n```{python}\n# Obtain data again\nX3 = hd['MedInc'].values\ny3 = hd['MedHouseVal'].values\nX3 = X3.reshape(-1, 1)\ny_pred3 = show_nonlin_regression(X3, y3, 3)\n```\n\n```{python}\n# Compute mean squared error\nmse = np.mean((y_pred2 - y2) ** 2)\nprint(f'Mean squared error: {mse}')\n```\n\nYet again, the model doesn't fit the data incredibly well, and the mean squared error is still quite high. Additionally, it appears that the model is begginning to overfit the data, as the line of best fit is beginning to curve too much to fit the data.\nIt doesn't make sense for very high income areas to have somewhat lower house prices, yet the model predicts this.\nWith simple regression it is difficult to model the relationship between median income and median house value, as there are many other factors that affect house prices. The variance in house prices is likely due to other factors, such as location, house size, and age of the house.\nHowever, we have still illustrated how linear and nonlinear regression can be applied to predict the value of future dependent variable observations, by modeling on recorded data.\n\nLastly, let's try to predict the median house value based on the other features, using multiple linear regression.\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Prepare feature matrix and target \nX = hd.drop(columns=['MedHouseVal'])  # Use all columns except 'MedHouseVal' as features\ny = hd['MedHouseVal']\n\n# Split dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and fit multiple linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Interpret the model coefficients\ncoefficients = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': model.coef_})\nintercept = model.intercept_\nprint(\"Intercept:\", intercept)\nprint(\"Coefficients:\")\nprint(coefficients)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Squared Error (MSE):\", mse)\nprint(\"R-squared (R2) Score:\", r2)\n```\n\nAs we can see, the multiple linear regression model fits the data much better than the simple linear regression model, and the mean squared error is much lower. \nIncorporating multiple features allows us to more closely model the real life relationships between these variables.\nNow lets use a final visualization for the work we have done.\n```{python}\n# Plot actual vs predicted vals\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, y_pred)\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')\nplt.title('Actual vs. Predicted Values')\nplt.show()\n```\n\nThere does appear to be a linear relationship between the actual and predicted values, following the equation y=x. This is a good sign, as it means our model is predicting the values reasonably. \nHowever we can see that the success of the regression was still limited by our choice of model, data features, and potentially outliers in the data.\n\n# Conclusion\nIn this blog post we have explored linear and nonlinear regression using Python, with the scikit-learn California housing dataset. Linear regression is a linear approach to modeling the relationship between a development variable and one or more explanatory variables, while nonlinear regression is a more general regression analysis in which data is fit to a model and then expressed as a mathematical function. \nRegression is a foundational topic to machine learning, and I hope this blog post has helped you understand some of the basics of regression and inspired you to learn more.\n\n## Sources used\n1. https://realpython.com/linear-regression-in-python/\n2. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html\n3. https://github.com/ageron/handson-ml3\n4. https://www.investopedia.com/terms/m/mlr.asp\n5. https://medium.com/machine-learning-with-python/multiple-linear-regression-implementation-in-python-2de9b303fc0c"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"flatly","title-block-banner":true,"title":"Linear & Nonlinear regression","author":"Jonathan West","date":"2023-11-23","categories":["Regression","code"],"description":"This is a blog post demonstrating linear and nonlinear regression."},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}