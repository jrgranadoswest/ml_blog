{
  "hash": "066c7bfc420398591c653252c6ffc5a6",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Jonathan West\"\ndate: \"2023-11-22\"\ncategories: [Clustering, code]\ndescription: \"This blog post demonstrates the concept of clustering in machine learning.\"\nimage: \"clustering.png\"\n---\n\n## Introduction\nClustering is an essential technique in machine learning and data analytics which involves grouping similar observations together into clusters. \nThe process is essential for finding patterns in data and is used in a variety of fields. \nIt is an unsupervised learning technique, meaning that it does not require labeled data, but rather finds patterns in the data itself. \n\nToday we will be demonstrating use of a couple different clustering algorithms.\n\n## K-Means\nK-Means clustering is a clustering algorithm which aims to partition \"n\" observations into \"k\" different groups. \nAs the name suggests, the grouping of each observation is determined by mean of the cluster. \nEssentially, the algorithm finds the mean of each cluster and assigns each observation to the cluster with the nearest mean, repeatedly recomputing the means until the clusters no longer change.\n\nLets exemplify this process with a fun example. We are going to use the K-Means algorithm to group the colors of an image into clusters. First, lets load an image.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n \n# Read in the image, convert to RGB\nimage = cv2.imread('landscape.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Reshaping the image into a 2D array of pixels and 3 float values (RGB)\npixels = image.reshape((-1,3))\npixels = np.float32(pixels)\n\n# Display image\nplt.imshow(image)\n```\n\n::: {.cell-output .cell-output-display execution_count=111}\n```\n<matplotlib.image.AxesImage at 0x7f107eaaed10>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=575 height=297}\n:::\n:::\n\n\nThis image is a matrix of pixels, where the pixels are represented by values that give their color based on the values of Red, Green, and Blue. \nTherefore each pixel can be considered as a point in a 3D color space, and we can use the K-Means algorithm to group the pixels in this space.\nTo be specific, each pixel will be grouped with the cluster whose mean is closest to the pixel's color, and these means will be updated until the clusters no longer change.\n\nThen, we can display the image with each pixel represented by the mean color of its cluster.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# run k-means with random initial centers, k=3 clusters, stopping at 100 iterations or an epsilon value of 90%\nk = 6\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.9)\nretval, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n \n# convert data into 8-bit values, and reshape to original image dimensions\ncenters = np.uint8(centers)\nsegmented_data = centers[labels.flatten()]\nsegmented_image = segmented_data.reshape((image.shape))\n\n# Display modified image \nplt.imshow(segmented_image)\n```\n\n::: {.cell-output .cell-output-display execution_count=112}\n```\n<matplotlib.image.AxesImage at 0x7f107c19f850>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=575 height=297}\n:::\n:::\n\n\nHere we can see the image after having run the K-Means algorithm with K=6 clusters. \nYou can probably think of some use cases for this, such as image compression, a stylistic effect, or simplifying an image for computer vision work or other analysis.\n\nTo further illustrate how k-means works, lets manually implement the algorithm on this data. Essentially we will be doing the same thing that the above code does: we initialize \"k\" different \ncluster centroids, assign each point to the cluster with the closest centroid, and then update the centroids to be the mean of the points in each cluster. \nWe will keep repeating this process until the centroids no longer change between iterations. \nWe start by initializing the centroids to random points in the color space. In practice we may want to ensure that the centroids are initialized to be far apart from each other or randomly sample existing colors in the image, but for simplicity we will just initialize them randomly. We also create a labels array to keep track of which cluster each pixel belongs to.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport random\nk = 4\n# Random number between 0 and 255:\nnum = random.randint(0, 255)\nmeans = np.array([[random.randint(0, 255) for _ in range(3)] for _ in range(k)])\nprint(means)\nlabels = np.zeros(pixels.shape[0], dtype=np.uint8)\nprint(pixels.shape)\nprint(labels.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 63 255 113]\n [ 57 154  88]\n [238   0 179]\n [ 29 141 112]]\n(110880, 3)\n(110880,)\n```\n:::\n:::\n\n\nNext, let's make a quick helper method that will reobtain the image from the pixels array and current cluster means.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef get_image(pixels, means):\n    new_pixels = np.zeros(pixels.shape, dtype=np.uint8)\n    for i, px in enumerate(pixels):\n        new_pixels[i] = means[labels[i]]\n    new_image = new_pixels.reshape((image.shape))\n    return new_image\n\ninit_image = get_image(pixels, means)  # image with initial random means\n```\n:::\n\n\nNow, we will implement the main program loop. For the sake of simplicity we will simply run 8 iterations, but in practice we would want to run until the centroids no longer change, or a certain epsilon value is reached.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Iterate over all pixels and assign them to the closest cluster\nidx = 0\nwhile idx < 8:\n    # Iterate over all pixels and update their cluster assignments\n    for i, px in enumerate(pixels):\n        # Compute euclidian distance (the l2 norm) to each cluster mean\n        distances = [np.linalg.norm(px - mean) for mean in means]\n        # New cluster is the one with the smallest distance\n        cluster = np.argmin(distances)\n        labels[i] = cluster\n    if idx == 0:\n        init_image = get_image(pixels, means)\n    elif idx == 1:\n        image_1_iter = get_image(pixels, means)\n    if idx == 3:\n        image_3_iter = get_image(pixels, means)\n    # Update cluster means\n    # find average of all pixels from \"pixels\" where the label corresponds to the current cluster mean\n    old_means = np.copy(means)\n    for i in range(k):\n        means[i] = np.mean(pixels[labels == i], axis=0)\n    # Compute difference between current and previous cluster means\n    idx += 1\n\nprint(means)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[212 184 161]\n [108  98  66]\n [153 140 118]\n [ 51  61  50]]\n```\n:::\n:::\n\n\nFinally, let's display the image results. We can display the image with each pixel represented by the mean color of its cluster, and show how the means changed over 0, 10, 50 and 100 iterations.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfinal_image = get_image(pixels, means)\nfig = plt.figure(figsize=(8, 8))\ncolumns = 2\nrows = 2\nfig.add_subplot(rows, columns, 1, title=\"Initial Clusters\")\nplt.imshow(init_image)\nfig.add_subplot(rows, columns, 2, title=\"After 1 Iteration\")\nplt.imshow(image_1_iter)\nfig.add_subplot(rows, columns, 3, title=\"After 3 Iterations\")\nplt.imshow(image_3_iter)\nfig.add_subplot(rows, columns, 4, title=\"Final Clusters\")\nplt.imshow(final_image)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=649 height=514}\n:::\n:::\n\n\nNow let's consider a more practical dataset for clustering. We are going to look at the scikit learn wine dataset, and work to cluster the wine samples into groups based on their features.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\n# Load the Wine dataset\nwine = load_wine()\nX = wine.data\ny_true = wine.target  # True class labels\n\nprint(\"Wine Dataset:\")\nprint(f\"{len(wine.target_names)} Target names: {', '.join(wine.target_names)}\")\nprint(f\"{len(wine.feature_names)} Feature names: {', '.join(wine.feature_names)}\")\nprint(f\"Data shape: {wine.data.shape}\")\nprint(wine.data[:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWine Dataset:\n3 Target names: class_0, class_1, class_2\n13 Feature names: alcohol, malic_acid, ash, alcalinity_of_ash, magnesium, total_phenols, flavanoids, nonflavanoid_phenols, proanthocyanins, color_intensity, hue, od280/od315_of_diluted_wines, proline\nData shape: (178, 13)\n[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n```\n:::\n:::\n\n\nThe dataset contains 13 features, and the target class includes 3 different types of wine.\nNote that because this data is not 2-dimensional, we will need to use a dimensionality reduction technique to visualize the clusters in a 2D plot. \nSpecifically, we will use PCA, or principle component analysis, to reduce the dimensionality of the data to 2D.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Apply PCA for dimensionality reduction, so we can visualize data with 2D graph\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Perform k-means clustering\nn_clusters = 3  # You can change the number of clusters as needed\nkmeans = KMeans(n_clusters=n_clusters, random_state=0)\nkmeans.fit(X)\n\n# Get cluster labels and cluster centers\nlabels = kmeans.labels_\ncenters = kmeans.cluster_centers_\n\nplt.figure(figsize=(8, 6))\n# Plot the data points colored by the cluster labels\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\nplt.title('K-Means Clustering Results')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/jrgw/CS5805_ML/blogposts/blogwork/jrgwblog/venv/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=686 height=523}\n:::\n:::\n\n\nAs we did before, we ran k-means clustering on the data, this time with k=3 clusters. \nTo plot the results we used principle component analysis (PCA) to reduce the dimensionality of the data to 2D, so we could visualize the clusters in a scatter plot.\n\nNext, lets plot the data with the three true classification labels they were provided with in the dataset, to compare to the unsupervised learning groups.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\n# Plot the data points colored by the true class labels\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_true, cmap='viridis')\nplt.title('True Class Labels')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=686 height=523}\n:::\n:::\n\n\nAs we can see from comparing the two graphs, the clusters found by the k-means algorithm are comparable to the true class labels, but not quite the same.\nNext, lets consider how the choice of K affects our results. In this particular case, we know that there are three true wine classes, so we would expect that the best choice of K would be 3.\nHowever in practice the number of classes based on what we are trying to accomplish may not be known, so we may need to experiment with different values of K to find the best one.\n\nWith the k-means algorithm, \"inertia\" is a measure that quantifies the compactness or tightness of the clusters. It calculates the sum of the squared distances between each data point and its corresponding centroid in its cluster. Inertia allows us to assess how well the data points are grouped into clusters, because lower inertia values indicate less distance from points to their cluster centroids, and therefore the clusters are more compact. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nkmeans_per_k = [KMeans(n_clusters=k, n_init=10, random_state=42).fit(wine.data) for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nplt.figure(figsize=(8, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.title(\"Elbow Curve\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=672 height=339}\n:::\n:::\n\n\nWe call this graph an \"Elbow Curve\", because the optimal value of K is often the point where the inertia curve has an \"elbow\" or sharp turn. \nAs you can see, as our choice of k increases, the inertia decreases because more data points allow us to have more compact clusters.\nThis will be true all the way until we reach k=n observations, where each observation is its own cluster and the inertia is 0.\nHowever, as you may have realized, this is not a good choice of k because it does not allow us to group similar observations together.\nThe optimal value of k is the point where the inertia curve has an \"elbow\" or sharp turn, because this is the point where the inertia decreases more slowly as k increases, and therefore the point where the clusters are most compact, while still meaningfully grouping the data.\n\nIn this case, it appears that the optimal value of k is 2 rather than the 3 true classes, because the inertia curve has a sharp turn at k=2, and then decreases more slowly as k increases. This may be due to the particular criteria of wine classes not perfectly matching the wine features that we are grouping based on, or it may be due to the fact that the wine classes are not perfectly distinct from each other. In practice, if we did not have the true class labels, then we would likely choose k=2 in this case.\n\n\n## Conclusion\nIn this blog post, we have jumped into the concept of clustering in machine learning, which involves using unsupervised learning to find patterns and groups in unlabeled data. We discussed k-means and how to implement it, and why it is used. By understanding the concept of inertia and its pivotal role in k-means optimization, we gained insights into how to best make use of it. Clustering is an essential technique with diverse applications, allowing us to extract meaningful patterns and structures from complex datasets, making it an essential tool in a machine learning practitioner's toolbox. Hopefully you can use this blog as an inspiration to explore clustering further, for example by trying other clustering algorithms such as DBSCAN or Gaussian Mixture Models, or by applying clustering to your own datasets.\n\n## References\n1. https://thepythoncode.com/article/kmeans-for-image-segmentation-opencv-python\n2. https://www.geeksforgeeks.org/image-segmentation-using-k-means-clustering/\n3. https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n4. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplot.html\n5. https://github.com/ageron/handson-ml3\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}