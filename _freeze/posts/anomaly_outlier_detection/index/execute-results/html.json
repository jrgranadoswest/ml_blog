{
  "hash": "358c1d5bdf99ca31c1e0eba1101c3463",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Jonathan West\"\ndate: \"2023-11-25\"\ncategories: [\"Outlier detection\", code]\ndescription: \"This blog demonstrates anomaly and outlier detection.\"\nformat:\n    html:\n        code-tools: true\n---\n\n## Introduction\nAnomaly detection is the process of identifying data points that are notably different from the rest of the data. \nObservations can be skewed due to erroneous data collection, a change in system behavior, or simply an unusual yet natural event. \nAnomalies/outliers can be applied to applications such as fraud detection or discovering defects in manufactured products. \nAdditionally, it is worth taking note of outlier detection because it can be important to remove outliers from data as a preprocessing step, for a subsequent machine learning algorithm to perform well.\n\nFirst, we are going to work with a \"toy\" dataset, by generating data from the scikit learn make_blobs function.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Create a dataset with clusters, and add some arbitrary values as outliers\nX, y = make_blobs(n_samples=400, centers=3, cluster_std=1.3, random_state=42)\noutliers = np.array([[7, 12], [1, -10], [9, 11]])\nX = np.vstack((X, outliers))\nX.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=112}\n```\n(403, 2)\n```\n:::\n:::\n\n\nThe make_blobs function lets us generate Gaussiang blobs for clustering. We generated 400 samples, with three centers and a standard deviation of 1.3. \nWe also added three arbitrarily chosen outlier points to the dataset, to practice anomaly detection.\nNext, let's fit a gaussian mixture model to the data, and plot the results.\n\n## Gaussian Mixture Model \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Fit a Gaussian Mixture Model\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X)\n\n# Predict the labels and probabilities\nlabels = gmm.predict(X)\nprobs = gmm.predict_proba(X)\n\n# Plot the data points, color-coded by cluster\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.colorbar()\n\nplt.title(\"Gaussian Mixture Model with Outliers\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=575 height=449}\n:::\n:::\n\n\nIt is fairly easy to visually identify the three outlier points that we added to the dataset. \nHowever, in a multi-dimensional dataset, it is not as easy to identify outliers, and in either case mathematical methods are needed to identify outliers and justify the classification.\nLet's identify the outliers in our generated data. \n\n## Identifying Outliers with Gaussian Mixture Models\n\nGaussian mixture models essentially work by fitting a number of gaussian distributions to the data, and then using the probabilities of each point belonging to each distribution to classify the points. \nIt is important to note that each observation in the data is therefore assigned a probability of belonging to each distribution. \nBy finding which observations have the lowest probability of belonging to any distribution, we can identify the outliers in the data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Set a threshold for outlier identification\nthreshold = 0.999\n\n# Find outliers based on the maximum probability of any distribution being below our cutoff\noutlier_indices = np.where(np.max(probs, axis=1) < threshold)\n\n# Plot the outliers\nplt.scatter(X[outlier_indices, 0], X[outlier_indices, 1], marker='o', color='r', s=100, label='Outliers')\n# Replot the rest of the data\nplt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\nplt.colorbar()\n```\n\n::: {.cell-output .cell-output-display execution_count=114}\n```\n<matplotlib.colorbar.Colorbar at 0x7f38758819d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=557 height=416}\n:::\n:::\n\n\nWe can see that two of the three outlier points we added are identified as outliers, and the third is not. \nAdditionally, the threshold was set quite high to consider the two points as outliers, and in many cases we would likely not consider them as anomalies in real data.\nLet's take a look at the lowest probability points in the data.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Find 10 data points with lowest probabilities\nlowest_prob_indices = np.argsort(np.max(probs, axis=1))[:10]\nfor i, idx in enumerate(lowest_prob_indices):\n    print(f\"{i+1}th lowest probability, idx {idx}; prob {np.max(probs[idx]):.4f}; point {X[idx]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1th lowest probability, idx 400; prob 0.9882; point [ 7. 12.]\n2th lowest probability, idx 402; prob 0.9982; point [ 9. 11.]\n3th lowest probability, idx 385; prob 0.9995; point [0.49985851 6.58684138]\n4th lowest probability, idx 223; prob 1.0000; point [2.92357567 4.35406608]\n5th lowest probability, idx 200; prob 1.0000; point [2.41280395 3.73320377]\n6th lowest probability, idx 232; prob 1.0000; point [3.57619195 4.69327314]\n7th lowest probability, idx 224; prob 1.0000; point [2.67013046 3.75010623]\n8th lowest probability, idx 398; prob 1.0000; point [-0.38706293  7.15510234]\n9th lowest probability, idx 174; prob 1.0000; point [2.64553041 3.63414955]\n10th lowest probability, idx 246; prob 1.0000; point [-0.63554235  7.14766533]\n```\n:::\n:::\n\n\nWe can see that the two outlier points we added are the two points with the lowest probabilities of belonging to any distribution, yet the third attempted outlier was not in the lowest 10 probabilities.\nThis may be because the third data point is closer to the overall center of the data, and therefore has a higher probability of belonging to one of the distributions, despite not being particularly close to any of the cluster centers. \nAdditionally, this highlights the importance of choosing an appropriate threshold and outlier for anomaly detection.\n\n## One Class SVM\nA single class support vector machine (SVM) is a supervised learning algorithm that is used to identify outliers in data.\nAs the name implies, it is a type of SVM that is designed to learn a particular boundary around a set of data, and then identify points that are outside of that boundary.\nIn other words, it finds a hyperplane in a high dimensional space that separates the majority of the data from potential outliers.\n\n\nLet's work with a more realistic dataset to practice this technique.\nWe will be examining the California housing dataset from scikit learn to explore the concept of anomaly and outlier detection. \n\n### Load and summarize the data\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California housing dataset\nhousing_data = fetch_california_housing()\nhd = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\nhd['target'] = housing_data.target\nhd.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=116}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.3252</td>\n      <td>41.0</td>\n      <td>6.984127</td>\n      <td>1.023810</td>\n      <td>322.0</td>\n      <td>2.555556</td>\n      <td>37.88</td>\n      <td>-122.23</td>\n      <td>4.526</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.3014</td>\n      <td>21.0</td>\n      <td>6.238137</td>\n      <td>0.971880</td>\n      <td>2401.0</td>\n      <td>2.109842</td>\n      <td>37.86</td>\n      <td>-122.22</td>\n      <td>3.585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.2574</td>\n      <td>52.0</td>\n      <td>8.288136</td>\n      <td>1.073446</td>\n      <td>496.0</td>\n      <td>2.802260</td>\n      <td>37.85</td>\n      <td>-122.24</td>\n      <td>3.521</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.6431</td>\n      <td>52.0</td>\n      <td>5.817352</td>\n      <td>1.073059</td>\n      <td>558.0</td>\n      <td>2.547945</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n      <td>3.413</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.8462</td>\n      <td>52.0</td>\n      <td>6.281853</td>\n      <td>1.081081</td>\n      <td>565.0</td>\n      <td>2.181467</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n      <td>3.422</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAs we have seen in a previous blog, this dataset contains housing data from California with the target variable being the median house value in hundreds of thousands of dollars.\nThere are eight features in this dataset. \n\nWe can visualize the data by using PCA for dimensionality reduction, and then plotting the first two principal components.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\n# Fit PCA to the data\npca = PCA(n_components=2)\npca.fit(housing_data.data)\n\n# Transform the data\nX_pca = pca.transform(housing_data.data)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=housing_data.target[:], cmap='plasma', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\ncol_bar = plt.colorbar()\ncol_bar.set_label('House Value')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=763 height=523}\n:::\n:::\n\n\nWe can see that there are some data points that definitely stand out from the rest of the data, based on principal component analysis. \nLet's get an evaluation of how well a linear regression model performs on this data, and then we will remove the outliers and see how the model performs.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n# split into input and output elements\nX = housing_data.data\ny = housing_data.target\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMAE: 0.534\n```\n:::\n:::\n\n\nNow we will use a one class SVM to identify the outliers.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# evaluate model performance with outliers removed using one class SVM\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.metrics import mean_absolute_error\n\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\n# identify outliers in the training dataset\nee = OneClassSVM(nu=0.01)\nyhat = ee.fit_predict(X_train)\n\n# select all rows that are not outliers\nmask = yhat != -1\nprint(\"Pre-outlier removal data shape:\", X_train.shape, y_train.shape)  # Shape of data before removing outliers\nX_train, y_train = X_train[mask, :], y_train[mask]\nprint(\"Post-outlier removal data shape:\", X_train.shape, y_train.shape)  # Shape of data after removing outliers\n\n# fit the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\nyhat = model.predict(X_test)\n# evaluate predictions\nmae = mean_absolute_error(y_test, yhat)\nprint('MAE: %.3f' % mae)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPre-outlier removal data shape: (13828, 8) (13828,)\nPost-outlier removal data shape: (13685, 8) (13685,)\nMAE: 0.505\n```\n:::\n:::\n\n\nIt appears that a significant number of outliers were removed from the data.\nWe can see that the mean absolute error of the model is reduced by removing the outliers from the data, however not by much in this case. \nFinally, we will make a plot of the data with the outliers removed.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\n# Fit PCA to the data\npca = PCA(n_components=2)\npca.fit(housing_data.data)\n\n# Transform the data\nX_pca = pca.transform(X_train)\n\n# Plot the data\nplt.figure(figsize=(10, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], s=10, c=y_train[:], cmap='plasma', marker='o', label='Feature 1')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('California Housing Dataset')\nplt.legend()\nplt.grid(True)\ncol_bar = plt.colorbar()\ncol_bar.set_label('House Value')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=766 height=523}\n:::\n:::\n\n\nIt is evident that with outliers removed, the data is much more tightly clustered, and this visualization allows us to view the data more clearly.\n\n## Conclusion\nIn this blog, we jumped into the concept of anomaly and outlier detection. \nThere are a variety of different techniques that can be used to identify outliers in data, and the choice of technique depends on the type of data and the application. \nIn this case, we practiced the use of Gaussian mixture models and one class SVMs to identify outliers in data.\nIf you are looking to further your knowledge on this subject, consider applying these techniques to a dataset of your choice, or exploring other techniques such as isolation forests or local outlier factor.\n\n## Sources used\n1. https://github.com/ageron/handson-ml3\n2. https://scikit-learn.org/stable/modules/outlier_detection.html\n3. https://machinelearningmastery.com/generate-test-datasets-python-scikit-learn/\n4. https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}